{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "“Predict Danmaku with Cloud TPUs and Keras”",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6ZDpd9XzFeN"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "KUu4vOt5zI9d",
        "colab": {}
      },
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "edfbxDDh2AEs"
      },
      "source": [
        "## Predict vtuber danmaku with Cloud TPUs and Keras\n",
        "#### Modified from \"Predict Shakespeare with Cloud TPUs and Keras\"\n",
        "Author github ID: pren1, coco401, simon3000, NeroArc, Afanyiyu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RNo1Vfghpa8j"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a Cloud TPU. This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
        "\n",
        "The model trains for 10 epochs and completes in approximately 1 hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QrprJD-R-410"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_I0RdnOSkNmi"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Train on TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kYxeFuKCUx9d"
      },
      "source": [
        "TPUs are located in Google Cloud, for optimal performance, they read data directly from Google Cloud Storage (GCS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lvo0t7XVIkWZ"
      },
      "source": [
        "## Data, model, and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xzpUtDMqmA-x"
      },
      "source": [
        "In this file, you train the model utilizing the danmaku data shown below:\n",
        "\n",
        "<blockquote>\n",
        "[\"完事\",\"了\",\"这\",\"是\",\"？\"],\n",
        "\n",
        "[\"来\",\"了\"],\n",
        "\n",
        "[\"哇\",\"我\",\"刚\",\"忙\",\"完\",\"o\",\"r\",\"z\"]\n",
        "</blockquote>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USsmqZM9n4T_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KRQ6Fjra3Ruq"
      },
      "source": [
        "### Download data\n",
        "You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpdTZVIYXaR",
        "colab_type": "code",
        "outputId": "a350936d-4b0a-49b0-cf59-3c74cd660e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1QWBjb9vk8TZhc9tZqxV1-BbVTj0GlMBy\n",
        "# !gdown https://drive.google.com/uc?id=1i6JH7x7SsAFYYX_EU1z5DHr1YQKeVyzN\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1bRf5YnXh8dkLwqgz4IdqaxRMKmzq0pxI\n",
        "# !gdown https://drive.google.com/uc?id=1jEo1ObjoHqI0JuPsCQuRndxw6xIPxjoR\n",
        "# !gdown https://drive.google.com/uc?id=1tBO5Bxfu3FRLLuudIQ_xHvi0t6LfO34m\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1DEVIMMeCLqtsiOKA3TgX2KRSDdvjhuYr\n",
        "# !gdown https://drive.google.com/uc?id=1T5OpFmiT00MFZYNHyGFXqpEcLvdIBVOr\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=169jYxkPev2lkfMy8eu497EuukLxXcMF-\n",
        "\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1V5juWnxQXwOOxJarxJ0V8-nVPF87QshX\n",
        "!gdown https://drive.google.com/uc?id=1B0UaIeixggEg30SUw3uvWxDGbabJo9NV\n",
        "!gdown https://drive.google.com/uc?id=1QLl2kqsPDoWhbmM22N9bdt1gOsuLRAdv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1V5juWnxQXwOOxJarxJ0V8-nVPF87QshX\n",
            "To: /content/glove-512-words.pkl\n",
            "100% 106k/106k [00:00<00:00, 25.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B0UaIeixggEg30SUw3uvWxDGbabJo9NV\n",
            "To: /content/glove-512.npy\n",
            "14.4MB [00:00, 126MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QLl2kqsPDoWhbmM22N9bdt1gOsuLRAdv\n",
            "To: /content/new_filtered_data.json\n",
            "235MB [00:01, 158MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1KCejQ7cBJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhoxvlM4cGlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls '/content/drive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15pAi9ShXdU",
        "colab_type": "text"
      },
      "source": [
        "## Process the data with index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmWOiUB4hV24",
        "colab_type": "code",
        "outputId": "00626f1a-ca29-4a2f-8666-f64401e6d4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pdb\n",
        "import collections\n",
        "import distutils\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from keras.utils import plot_model\n",
        "\n",
        "def remove_rare_characters(obtain_freq, min_times):\n",
        "    'Remove characters occur less than min_times'\n",
        "    freq_characters = []\n",
        "    rare_characters = []\n",
        "    print(\"processing character dictionary...\")\n",
        "    for i in tqdm(range(len(obtain_freq))):\n",
        "        char, freq_num = obtain_freq[i]\n",
        "        if freq_num > min_times:\n",
        "            freq_characters.append(char)\n",
        "        else:\n",
        "            rare_characters.append(char)\n",
        "    return freq_characters, rare_characters\n",
        "\n",
        "def process_data_with_index(txt, minimum_occur_time):\n",
        "    text = (open(txt).read())\n",
        "    text = text.lower()\n",
        "    characters = sorted(list(set(text)), reverse=True)\n",
        "    'at here, remove character occurs less than minimum_occur_time'\n",
        "    obtain_freq = collections.Counter(text).most_common()\n",
        "    freq_characters, rare_characters = remove_rare_characters(obtain_freq, minimum_occur_time)\n",
        "    print(\"freq char: {}\\{}, rare char: {}\\{}\".format(len(freq_characters), len(characters), len(rare_characters), len(characters)))\n",
        "    return freq_characters, rare_characters, text\n",
        "\n",
        "# 'the character should occur this much time if they wanna to be taken into account'\n",
        "# minimum_occur_time = 100\n",
        "context_vector_length = 100\n",
        "context_seq_length = 130\n",
        "batch_size = 2048\n",
        "# SHAKESPEARE_TXT = '/content/bert-master_danmaku_text_pure.txt'\n",
        "# 'Use the following path to just save you some time'\n",
        "# preprocessed_TXT = '/content/rectified_input.txt'\n",
        "# characters, rare_characters, input_text = process_data_with_index(SHAKESPEARE_TXT, minimum_occur_time)\n",
        "\n",
        "'load in characters, and embedding matrix'\n",
        "with open('/content/glove-512-words.pkl', 'rb') as f:\n",
        "    characters = pickle.load(f)\n",
        "    'also add end part, and beginning part'\n",
        "    characters[-1] = 'eos'\n",
        "    characters[0] = '\\n'\n",
        "preprocessed_TXT = '/content/new_filtered_data.json'\n",
        "embedding_matrix = np.load('/content/glove-512.npy')\n",
        "'show something about embedding'\n",
        "\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "\n",
        "def transform(txt):\n",
        "    return np.asarray([char_to_n[c] for c in txt], dtype=np.int32)\n",
        "\n",
        "# def remove_unkown_character_from_text(txt, rare_characters):\n",
        "#     'Remove char in rare_characters from txt' \n",
        "#     for x in tqdm(rare_characters):\n",
        "#         try:\n",
        "#             txt = txt.replace(x, \"\")\n",
        "#         except ValueError:\n",
        "#             pass\n",
        "#     return txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3V4V-Jxmuv3",
        "colab": {}
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "import json\n",
        "def input_fn(seq_len=context_seq_length, batch_size=batch_size):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "    data = json.load(json_file, encoding='UTF-8')\n",
        "    'process the data'\n",
        "    txt = []\n",
        "    for single_meg in data:\n",
        "      single_meg.insert(0, 'eos')\n",
        "      single_meg.append('\\n')\n",
        "      txt.extend(single_meg)\n",
        "    'remove that does not belongs to characters...'\n",
        "    new_txt = []\n",
        "    for sing in tqdm(txt):\n",
        "      sing = sing.lower()\n",
        "      skip_this = False\n",
        "      if sing != '鸨儿':\n",
        "        new_txt.append(sing)\n",
        "    print(\"updated txt, remove from {} to {}, examples: {}\".format(len(txt), len(new_txt), new_txt[:20]))\n",
        "    print(\"Processing the txt: {}\".format(txt[1000:1200]))\n",
        "    txt = new_txt\n",
        "  #   pdb.set_trace()\n",
        "  #   pdb.set_trace()\n",
        "  # with tf.io.gfile.GFile(preprocessed_TXT, 'r') as f:\n",
        "  #   txt = f.read()\n",
        "  #   txt = txt.lower()\n",
        "  #   txt = txt.replace('\\r','')\n",
        "  #   pdb.set_trace()\n",
        "    'If the input is preprocessed_TXT, then you do not need this one'\n",
        "    # txt = remove_unkown_character_from_text(txt, rare_characters)\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    context_vector = chunk[:context_vector_length]\n",
        "    input_text = chunk[context_vector_length:-1]\n",
        "    target_text = chunk[context_vector_length+1:]\n",
        "    return (context_vector, input_text), target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "  return ds.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bbb05dNynDrQ"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
        "\n",
        "The input dimension to the Embedding layer is the same as our vocabulary size.\n",
        "\n",
        "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLEM-fLJlEEt",
        "outputId": "f04183d2-3f6a-43c4-cd70-d35bab4b06dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "HALF_EMBEDDING_DIM = int(EMBEDDING_DIM/2)\n",
        "regularizer_coefficient = 0.00001\n",
        "def lstm_model(seq_len=30, context_length = context_vector_length, batch_size=None, stateful=True):\n",
        "    \"\"\"Language model: Encoder decoder favor for context term\"\"\"\n",
        "    encoder_input = tf.keras.Input(name='Encoder_input', shape=(context_length,), batch_size=batch_size, dtype=tf.int32)\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim=len(characters), output_dim=EMBEDDING_DIM, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)\n",
        "    encode_embedding = embedding_layer(encoder_input)\n",
        "    enc_lstm1, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HALF_EMBEDDING_DIM, name='encoder_lstm_1', return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))(encode_embedding)\n",
        "    state_h_1 = tf.keras.layers.concatenate([forward_h, backward_h])\n",
        "    state_c_1 = tf.keras.layers.concatenate([forward_c, backward_c])\n",
        "    enc_lstm1 = tf.keras.layers.Dropout(0.6)(enc_lstm1)\n",
        "    encoder_states_1 = [state_h_1, state_c_1]\n",
        "\n",
        "    enc_lstm2, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HALF_EMBEDDING_DIM, name='encoder_lstm_2', return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))(enc_lstm1)\n",
        "    state_h_2 = tf.keras.layers.concatenate([forward_h, backward_h])\n",
        "    state_c_2 = tf.keras.layers.concatenate([forward_c, backward_c])\n",
        "    encoder_states_2 = [state_h_2, state_c_2]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = tf.keras.Input(name='Decoder_input', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    lstm_1_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_1', stateful=stateful, return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient))\n",
        "    lstm_1, _, _ = lstm_1_layer(decode_embedding, initial_state=encoder_states_1)\n",
        "    dropout_lstm_1 = tf.keras.layers.Dropout(0.6)(lstm_1)\n",
        "    lstm_2_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_2', stateful=stateful, return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient))\n",
        "    lstm_2, _, _  = lstm_2_layer(dropout_lstm_1, initial_state=encoder_states_2)\n",
        "    dropout_lstm_2 = tf.keras.layers.Dropout(0.6)(lstm_2)\n",
        "\n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([dropout_lstm_2, enc_lstm2])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, enc_lstm2])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, dropout_lstm_2])\n",
        "\n",
        "    dense_layer_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(EMBEDDING_DIM*4, activation='tanh' , kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))\n",
        "    predicted_char_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(characters), activation='softmax' , kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))\n",
        "\n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    predicted_char = predicted_char_layer(dense_layer_output_1)\n",
        "    \n",
        "    Model = tf.keras.Model(inputs=[encoder_input, decoder_inputs], outputs=[predicted_char])\n",
        "    # Model.summary()\n",
        "    tf.keras.utils.plot_model(Model, show_shapes=True, to_file='model.png')\n",
        "\n",
        "    'For reference, also prepared some tricks'\n",
        "    encoder_model = tf.keras.Model(encoder_input, [encoder_states_1[0], encoder_states_1[1], encoder_states_2[0], encoder_states_2[1], enc_lstm2])\n",
        "    tf.keras.utils.plot_model(encoder_model, show_shapes=True, to_file='encoder_model.png')\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_h1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    \n",
        "    encoder_output_in = tf.keras.Input(shape=(context_vector_length, EMBEDDING_DIM,))\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, decoder_state_input_h1, decoder_state_input_c1]\n",
        "    d_o, state_h, state_c = lstm_1_layer(decode_embedding, initial_state=decoder_states_inputs[:2])\n",
        "    d_o, state_h1, state_c1 = lstm_2_layer(d_o, initial_state=decoder_states_inputs[-2:])\n",
        "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
        "\n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([d_o, encoder_output_in])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, encoder_output_in])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, d_o])\n",
        "\n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    decoder_outputs = predicted_char_layer(dense_layer_output_1)\n",
        "    decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs + [encoder_output_in], [decoder_outputs] + decoder_states)\n",
        "    tf.keras.utils.plot_model(decoder_model, show_shapes=True, to_file='decoder_model.png')\n",
        "\n",
        "    return Model, encoder_model, decoder_model\n",
        "\n",
        "def get_stand_alone_decoder(seq_len=30, context_length = context_vector_length, batch_size=None, stateful=True):\n",
        "    decoder_state_input_h = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_h1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    \n",
        "    decoder_inputs = tf.keras.Input(name='Decoder_input', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "    \n",
        "    encoder_output_in = tf.keras.Input(shape=(context_vector_length, EMBEDDING_DIM,))\n",
        "\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim=len(characters), output_dim=EMBEDDING_DIM, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, decoder_state_input_h1, decoder_state_input_c1]\n",
        "    lstm_1_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_1', stateful=stateful, return_state=True, return_sequences=True)\n",
        "    d_o, state_h, state_c = lstm_1_layer(decode_embedding, initial_state=decoder_states_inputs[:2])\n",
        "    lstm_2_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_2', stateful=stateful, return_state=True, return_sequences=True)\n",
        "    d_o, state_h1, state_c1 = lstm_2_layer(d_o, initial_state=decoder_states_inputs[-2:])\n",
        "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
        "    \n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([d_o, encoder_output_in])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, encoder_output_in])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, d_o])\n",
        "    \n",
        "    dense_layer_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(EMBEDDING_DIM*4, activation='tanh'))\n",
        "    predicted_char_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(characters), activation='softmax'))\n",
        "    \n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    decoder_outputs = predicted_char_layer(dense_layer_output_1)\n",
        "    decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs + [encoder_output_in], [decoder_outputs] + decoder_states)\n",
        "    tf.keras.utils.plot_model(decoder_model, show_shapes=True, to_file='decoder_model.png')\n",
        "    return decoder_model\n",
        "\n",
        "def step_decay(epoch):\n",
        "    import math\n",
        "    initial_lrate = 0.001\n",
        "    drop = 0.6\n",
        "    epochs_drop = 1.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
        "    print(\"lrate: {}, epoch: {}\".format(lrate, epoch))\n",
        "    return lrate\n",
        "\n",
        "training_model,encoder_model, decoder_model = lstm_model(seq_len=30, context_length = context_vector_length, stateful=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0817 16:06:11.041363 140644906526592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0817 16:06:11.056726 140644906526592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0817 16:06:11.062697 140644906526592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0817 16:06:11.064177 140644906526592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0817 16:06:11.659091 140644906526592 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0817 16:06:12.357251 140644906526592 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0817 16:06:12.722395 140644906526592 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VzBYDJI0_Tfm"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "First, we need to create a distribution strategy that can use the TPU. In this case it is TPUStrategy. You can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods `fit`, `evaluate` and `predict` use the TPU.\n",
        "\n",
        "Again note that we train with `stateful=False` because while training, we only care about one batch at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExQ922tfzSGA",
        "outputId": "8a2f202d-f22f-4881-8ec1-5512d650a64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  training_model,encoder_model, decoder_model = lstm_model(seq_len=30, context_length = context_vector_length, stateful=False)\n",
        "  lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "  adam = tf.keras.optimizers.RMSprop(lr=0.0, decay=0.0)\n",
        "  training_model.compile(\n",
        "      # optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      optimizer = 'adam',\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0817 16:06:17.178209 140644906526592 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0817 16:06:26.625607 140644906526592 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0817 16:06:27.811969 140644906526592 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec2YZwjojEye",
        "colab_type": "code",
        "outputId": "9f1b7e53-f8ea-41b5-e6e2-c8540340c066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=30000,\n",
        "    epochs=1,\n",
        "    callbacks=[lrate]\n",
        ")\n",
        "saver_index = 0\n",
        "training_model.save_weights('/tmp/bard_{}.h5'.format(saver_index), overwrite=True)\n",
        "#training_model.save('/tmp/bard_{}.js.h5'.format(saver_index), overwrite=True)\n",
        "saver_index += 100"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31596186/31596186 [00:19<00:00, 1586825.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "updated txt, remove from 31596186 to 31596023, examples: ['eos', '待', '機', '\\n', 'eos', '.', '.', '\\n', 'eos', '\\r', '\\n', 'eos', '打卡', '\\n', 'eos', '?', '\\n', 'eos', '(', '=']\n",
            "Processing the txt: ['了', '\\n', 'eos', '再次', '坑', '乌拉', '\\n', 'eos', 'p', 'o', 'i', '和', '乌拉', '拉', '在', '一起', '打', '吗', '？', '\\n', 'eos', '马赛克', '！', '\\n', 'eos', '噗', '\\n', 'eos', 'e', 'm', 'm', 'm', '火车', '晚', '了', 'p', 'o', 'i', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '你', '要', '知道', '我', '写', '过', '文', '\\n', 'eos', '咬', '滑稽', '\\n', 'eos', '这', '是', '一段', '有', '画面', '的', '文字', '\\n', 'eos', '污', '拉拉', '\\n', 'eos', '乌拉', '拉', '晚上', '好', '\\n', 'eos', '去', '拿', '对面', '油', '\\n', 'eos', '晚', '好', '吖', '\\n', 'eos', '哇', '，', '乌拉', '粉丝', '还有', '看', '高', '达', '的', '\\n', 'eos', '（', '6', '9', '）', '\\n', 'eos', '我', '怀疑', '你', '在', '开车', '\\n', 'eos', '尝试', '理解', 'p', 'o', 'i', '\\n', 'eos', '好多', '.', '。', '。', '。', '\\n', 'eos', '不要', '火车', 'p', 'o', 'i', '？', '\\n', 'eos', '上次', '。', '。', '。', '\\n', 'eos', '我', '看看', '我', '下', '个', 'c', 'o', 'h', '\\n', 'eos', '穿', '模', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '那个']\n",
            "lrate: 0.0006, epoch: 0\n",
            "30000/30000 [==============================] - 6603s 220ms/step - loss: 2.4742 - sparse_categorical_accuracy: 0.5834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV3mrS_xjDgZ",
        "colab_type": "text"
      },
      "source": [
        "# 删除EOS 和\\n  &  分割 语句"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfXgJWGTjBd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_EOS(input: list) -> list:\n",
        "    while 'eos' in input:\n",
        "        input.remove('eos')\n",
        "    str1 = \"\".join(input)\n",
        "    res = str1.split('\\n')\n",
        "    del res[-1]\n",
        "    for single in res:\n",
        "      print(single)\n",
        "    return res  \n",
        "# print(delete_EOS(['eos', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '观看', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '转播man', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '转播man', '\\n', 'eos', 'a', 'n', 't', 'i', '路过', '，', '不用', '管', '\\n', 'eos', 'o', 'k', 'k', '\\n', 'eos', 'a', 'n', 't', 'i', '需要', '理由', '吗', '？', '\\n']\n",
        "# ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwiZBc6dmIzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'get some real text inputs'\n",
        "import random\n",
        "import copy\n",
        "\n",
        "def load_in_texts():\n",
        "  with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "      data = json.load(json_file, encoding='UTF-8')\n",
        "      'process the data'\n",
        "      txt = []\n",
        "      for single_meg in data:\n",
        "        single_meg.insert(0, 'eos')\n",
        "        single_meg.append('\\n')\n",
        "        txt.extend(single_meg)\n",
        "      'remove that does not belongs to characters...'\n",
        "      new_txt = []\n",
        "      for sing in tqdm(txt):\n",
        "        sing = sing.lower()\n",
        "        skip_this = False\n",
        "        if sing != '鸨儿':\n",
        "          new_txt.append(sing)\n",
        "      print(\"updated txt, remove from {} to {}, examples: {}\".format(len(txt), len(new_txt), new_txt[:20]))\n",
        "      return new_txt\n",
        "\n",
        "def clip_text(txt_length, new_txt):\n",
        "    start_index = random.randint(0,len(new_txt) - txt_length)\n",
        "    clipped_txt_for_test = new_txt[start_index:start_index + txt_length]\n",
        "    delete_EOS(copy.deepcopy(clipped_txt_for_test))\n",
        "    return clipped_txt_for_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQ7ymktLXek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder_model.save_weights('/content/drive/My Drive/encoder.h5')\n",
        "# decoder_model.save_weights('/content/drive/My Drive/decoder.h5')\n",
        "\n",
        "encoder_model.save_weights('/tmp/encoder.h5')\n",
        "decoder_model.save_weights('/tmp/decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TCBtcpZkykSf"
      },
      "source": [
        "### Make predictions with the model\n",
        "\n",
        "Use the trained model to make predictions and generate your own fake danmaku messages.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
        "\n",
        "The predictions are done on the CPU so the batch size (5) in this case does not have to be divisible by 8.\n",
        "\n",
        "Note that when we are doing predictions or, to be more precise, text generation, we set `stateful=True` so that the model's state is kept between batches. If stateful is false, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
        "\n",
        "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"草\" and the output probabilities are \"草\" (0.65), \"哈\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"草\" and \"哈\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCzuRE6lLzUh",
        "colab_type": "code",
        "outputId": "05231352-45b7-4c45-e048-116d5751f28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "!pip install pprint"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pprint\n",
            "  Downloading https://files.pythonhosted.org/packages/99/12/b6383259ef85c2b942ab9135f322c0dce83fdca8600d87122d2b0181451f/pprint-0.1.tar.gz\n",
            "Building wheels for collected packages: pprint\n",
            "  Building wheel for pprint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pprint: filename=pprint-0.1-cp36-none-any.whl size=1251 sha256=0835d14c477ad926efabd5e91d8ba32a54f4838a4d06946d77f08dc805d50a7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/d4/c6/16a6495aecc1bda5d5857bd036efd50617789ba9bea4a05124\n",
            "Successfully built pprint\n",
            "Installing collected packages: pprint\n",
            "Successfully installed pprint-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tU7M-EGGxR3E",
        "colab": {}
      },
      "source": [
        "# BATCH_SIZE = 2\n",
        "# PREDICT_LEN = 30\n",
        "# BEAM_SIZE = 10\n",
        "# import pprint\n",
        "# # Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# # We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# # time and predicting the next character.\n",
        "# # tf.keras.backend.clear_session()\n",
        "\n",
        "# _, encoder_model, _ = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# # encoder_model.load_weights('/content/drive/My Drive/encoder.h5')\n",
        "# encoder_model.load_weights('/tmp/encoder.h5')\n",
        "\n",
        "# decoder_model = get_stand_alone_decoder(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# # decoder_model.load_weights('/content/drive/My Drive/decoder.h5')\n",
        "# decoder_model.load_weights('/tmp/decoder.h5')\n",
        "\n",
        "# # We seed the model with our initial string, copied BATCH_SIZE times\n",
        "# # seed_txt = ['了', '\\n', 'eos', '再次', '坑', '乌拉', '\\n', 'eos', 'p', 'o', 'i', '和', '乌拉', '拉', '在', '一起', '打', '吗', '？', '\\n', 'eos', '马赛克', '！', '\\n', 'eos', '噗', '\\n', 'eos', 'e', 'm', 'm', 'm', '火车', '晚', '了', 'p', 'o', 'i', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '你', '要', '知道', '我', '写', '过', '文', '\\n', 'eos', '咬', '滑稽', '\\n', 'eos', '这', '是', '一段', '有', '画面', '的', '文字', '\\n', 'eos', '污', '拉拉', '\\n', 'eos', '乌拉', '拉', '晚上', '好', '\\n', 'eos', '去', '拿', '对面', '油', '\\n', 'eos', '晚', '好', '吖', '\\n', 'eos', '哇', '，', '乌拉', '粉丝', '还有', '看', '高', '达', '的', '\\n', 'eos', '（', '6', '9', '）', '\\n', 'eos', '我', '怀疑', '你', '在', '开车', '\\n', 'eos', '尝试', '理解', 'p', 'o', 'i', '\\n', 'eos', '好多', '.', '。', '。', '。', '\\n', 'eos', '不要', '火车', 'p', 'o', 'i', '？', '\\n', 'eos', '上次', '。', '。', '。', '\\n', 'eos', '我', '看看', '我', '下', '个', 'c', 'o', 'h', '\\n', 'eos', '穿', '模', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '天狗', '\\n']\n",
        "# # seed_txt = ['eos','天', '狗','\\n']*200\n",
        "\n",
        "# print(\"Load in texts...\")\n",
        "# seed = transform(load_in_texts(100))\n",
        "# # print(seed_txt)\n",
        "\n",
        "# # seed = transform(load_in_texts(100))\n",
        "# seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "# # Encode the input as state vectors.\n",
        "# state_and_output = encoder_model.predict(seed)\n",
        "# states_value = [state_and_output[:4]] * BEAM_SIZE\n",
        "# encoder_output = state_and_output[-1]\n",
        "\n",
        "# # Solve decoder things\n",
        "# last_predictions = [np.array([[7010]]*BATCH_SIZE, dtype=np.int32)]\n",
        "# # Beam serach impl!\n",
        "# 'at first, all prob is 0'\n",
        "# path_saver = [[0, list()]] * BEAM_SIZE\n",
        "# print(\"Preforming beam search...\")\n",
        "# for i in tqdm(range(PREDICT_LEN)):\n",
        "#   total_slot = []\n",
        "#   for beam_words_id in range(len(last_predictions)):\n",
        "#     'for this words'\n",
        "#     last_word = last_predictions[beam_words_id]\n",
        "#     next_probits, h, c, h1, c1 = decoder_model.predict([last_word] + states_value[beam_words_id] + [encoder_output])\n",
        "#     'assign right states value'\n",
        "#     if len(last_predictions) == 1:\n",
        "#       'at the beginning, just renew all the state values'\n",
        "#       for i in range(BEAM_SIZE):\n",
        "#         states_value[i] = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "#     else:\n",
        "#       'if we have more choices, only update one'\n",
        "#       states_value[beam_words_id] = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "#     batch_id = 0\n",
        "#     next_probits = next_probits[:, 0, :][batch_id]\n",
        "#     'for each batch'\n",
        "#     'just a dirty work around, since all batch return the same results'\n",
        "#     previous_prob = path_saver[beam_words_id]\n",
        "#     top_k_words = next_probits.argsort()[-BEAM_SIZE:]\n",
        "#     for words_id in top_k_words:\n",
        "#       total_slot.append([previous_prob[0] + np.log(next_probits[words_id]), previous_prob[1] + [words_id]])\n",
        "#   'sort by the first prob'\n",
        "#   path_saver = sorted(total_slot, key=lambda tup:tup[0])[-BEAM_SIZE:]\n",
        "#   last_predictions = []\n",
        "  \n",
        "#   'Do something to get last predictions work here'\n",
        "#   for previous_path_tuple in path_saver:\n",
        "#     last_path = previous_path_tuple[1][-1]\n",
        "#     last_predictions.append(np.array([[last_path]]*BATCH_SIZE, dtype=np.int32))\n",
        "\n",
        "# 'generate top k sentences'\n",
        "# fin_res = []\n",
        "# for single_path in path_saver:\n",
        "#   prob = single_path[0]\n",
        "#   sentence = []\n",
        "#   for val in single_path[1]:\n",
        "#     current_char = n_to_char[val]\n",
        "#     sentence.append(current_char)\n",
        "#     if current_char == '\\n':\n",
        "#       break\n",
        "#   generated_sentence = ''.join(sentence)  # Convert back to text\n",
        "#   fin_res.append([prob, generated_sentence])\n",
        "\n",
        "# fin_res = sorted(fin_res, key=lambda tup:tup[0])\n",
        "# pprint.pprint(fin_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9GdlJWwpaFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "PREDICT_LEN = 50\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "_, encoder_model, _ = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# prediction_model.load_weights('/tmp/bard_{}.h5'.format(0))\n",
        "encoder_model.load_weights('/tmp/encoder.h5')\n",
        "\n",
        "decoder_model = get_stand_alone_decoder(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "decoder_model.load_weights('/tmp/decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etpePQVKpdRN",
        "colab_type": "code",
        "outputId": "29c140b0-6bbf-42b7-f63f-77219a6fd086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "# seed_txt = ['那', '是', '你', '心态', '不行', '\\n', 'eos', '我', '爱', '酱', '真是', '越来越', '聪明', '啦', '？', '\\n', 'eos', '本子', '预定', '\\n', 'eos', '被', '吓', '到', '了', '\\n', 'eos', '哈哈哈', '哈哈哈', '\\n', 'eos', '吹', '\\n', 'eos', '字幕', '没错', '好', '吧', '\\n', 'eos', '代', '打', '当然', '是', '开玩笑', '，', '但是', '说', '多', '了', '是', '真的', '烦', '\\n', 'eos', '代', '打', '是', '不过', '分', '，', '但', '你', '一直', '刷', '，', '你', '不', '烦', '别人', '烦', '\\n', 'eos', '整个', '游戏', '就', '在', '这儿', '卡', '关', '了', '不', '知道', '可以', '跳', '2', '3', '3', '3', '\\n', 'eos', '1', '7', '秒', '\\n', 'eos', 'w', 'w', 'w', 'w']\n",
        "# seed_txt = ['嘛', '（', '清楚', '多', '意', '）', '\\n', 'eos', '完事', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', '很', '懂', '\\n', 'eos', '自己', '都', '笑', '了', '\\n', 'eos', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', '老鸨', '\\n', 'eos', '很', '懂', '。', '。', '。', '我', '第一次', '知道', '\\n', 'eos', 'j', 'k', '一周', '目', '草', '生', '\\n', 'eos', '过于', '清楚', '\\n', 'eos', '艾', '琳', '太', '真实', '了', '\\n', 'eos', '实在', '是', '过于', '清楚', '\\n', 'eos', '怎么', '知道', '的']\n",
        "# seed = transform(seed_txt)\n",
        "new_txt = load_in_texts()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31596186/31596186 [00:22<00:00, 1425346.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "updated txt, remove from 31596186 to 31596023, examples: ['eos', '待', '機', '\\n', 'eos', '.', '.', '\\n', 'eos', '\\r', '\\n', 'eos', '打卡', '\\n', 'eos', '?', '\\n', 'eos', '(', '=']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4ZpIuhNYOQ",
        "colab_type": "code",
        "outputId": "f3233df4-ef49-46b6-be5c-ffcaae99f09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Load in texts...\")\n",
        "seed = transform(clip_text(100, new_txt))\n",
        "print(\"generating text...\")\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "# Encode the input as state vectors.\n",
        "state_and_output = encoder_model.predict(seed)\n",
        "states_value = state_and_output[:4]\n",
        "encoder_output = state_and_output[-1]\n",
        "# Solve decoder things\n",
        "predictions = [np.array([[7010]]*BATCH_SIZE, dtype=np.int32)]\n",
        "predictions_prob = []\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits, h, c, h1, c1 = decoder_model.predict([last_word] + states_value + [encoder_output])\n",
        "  next_probits = next_probits[:, 0, :]\n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(len(characters), p=next_probits[i])\n",
        "      # np.argmax(next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  'build the prob case'\n",
        "  prob = []\n",
        "  for batch_id in range(BATCH_SIZE):\n",
        "    prob.append(next_probits[batch_id][next_idx[batch_id]])\n",
        "  predictions_prob.append(np.asarray(prob))\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  # Update states\n",
        "  states_value = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "\n",
        "generated_whole_list = []\n",
        "for i in range(BATCH_SIZE):\n",
        "  # print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  p_prob = [predictions_prob[j][i] for j in range(PREDICT_LEN)]\n",
        "  current_list = []\n",
        "  'one sentence for one batch'\n",
        "  this_batch_prob = 0.\n",
        "  for index in range(len(p)):\n",
        "    'just get the character generated'\n",
        "    val = p[index]\n",
        "    cur_prob = np.log(p_prob[index])\n",
        "    if index == 0:\n",
        "      val = val[0]\n",
        "    current_char = n_to_char[val]\n",
        "    current_list.append(current_char)\n",
        "    this_batch_prob += cur_prob\n",
        "    if current_char == '\\n':\n",
        "      break\n",
        "  'we also wanna the average prob here'\n",
        "  this_batch_prob/=len(current_list)\n",
        "  current_list.remove('eos')\n",
        "  generated = ''.join(current_list)  # Convert back to text\n",
        "  generated_whole_list.append([this_batch_prob, generated])\n",
        "fin_res = sorted(generated_whole_list, key=lambda tup:tup[0], reverse=True)\n",
        "\n",
        "for this_batch_prob, generated in fin_res:\n",
        "  print(\"with prob: {}, generated: {}\".format(this_batch_prob, generated))\n",
        "# assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load in texts...\n",
            "66666666\n",
            "666666666666666\n",
            "草\n",
            "斯国一\n",
            "夏色祭的消失\n",
            "有能\n",
            "带制作\n",
            "6666666666\n",
            "带制作\n",
            "66666666666666\n",
            "xgnb！！！！！！！！！\n",
            "generating text...\n",
            "with prob: -0.34075465499024304, generated: 88888888888888888888888888\n",
            "\n",
            "with prob: -0.393750019290525, generated: 888888888888888888888\n",
            "\n",
            "with prob: -0.4197937681085684, generated: 88888888888888888888\n",
            "\n",
            "with prob: -0.48133267778515193, generated: 6666666\n",
            "\n",
            "with prob: -0.48133267778515193, generated: 6666666\n",
            "\n",
            "with prob: -0.48133267778515193, generated: 6666666\n",
            "\n",
            "with prob: -0.48133267778515193, generated: 6666666\n",
            "\n",
            "with prob: -0.48133267778515193, generated: 6666666\n",
            "\n",
            "with prob: -0.5120421830075793, generated: 强高倍！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
            "with prob: -0.5678837405155517, generated: 是糖！！！！！！！！！！！！！！\n",
            "\n",
            "with prob: -0.6069983663037419, generated: ？？？？？？？？\n",
            "\n",
            "with prob: -0.6069983663037419, generated: ？？？？？？？？\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6477108449675143, generated: tql\n",
            "\n",
            "with prob: -0.6531313830055296, generated: 8888888888\n",
            "\n",
            "with prob: -0.6531313830055296, generated: 8888888888\n",
            "\n",
            "with prob: -0.8057952507709464, generated: nb！！！！！\n",
            "\n",
            "with prob: -0.8390444443405917, generated: 牛逼！！！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.859318158775568, generated: ！！！\n",
            "\n",
            "with prob: -0.9397281876632145, generated: 牛逼！！！\n",
            "\n",
            "with prob: -0.9397281876632145, generated: 牛逼！！！\n",
            "\n",
            "with prob: -0.95525510632433, generated: ？？？？？？\n",
            "\n",
            "with prob: -0.9632330539636313, generated: www\n",
            "\n",
            "with prob: -0.9805350023536727, generated: 支持！！！！！！！！！！\n",
            "\n",
            "with prob: -0.9851391535333317, generated: 体验冠军！！！！！！！！！！！！！！！！！！！\n",
            "\n",
            "with prob: -0.9852601766586304, generated: nb！\n",
            "\n",
            "with prob: -0.9852601766586304, generated: nb！\n",
            "\n",
            "with prob: -0.9852601766586304, generated: nb！\n",
            "\n",
            "with prob: -0.9924058785662055, generated: 太强了\n",
            "\n",
            "with prob: -0.9924058785662055, generated: 太强了\n",
            "\n",
            "with prob: -0.9924058785662055, generated: 太强了\n",
            "\n",
            "with prob: -0.9924058785662055, generated: 太强了\n",
            "\n",
            "with prob: -0.9924058785662055, generated: 太强了\n",
            "\n",
            "with prob: -1.0289142689268505, generated: 也太强了吧\n",
            "\n",
            "with prob: -1.0582379925633885, generated: gachi恋距离\n",
            "\n",
            "with prob: -1.0689076215028763, generated: 真厉害啊\n",
            "\n",
            "with prob: -1.0689076215028763, generated: 真厉害啊\n",
            "\n",
            "with prob: -1.0689076215028763, generated: 真厉害啊\n",
            "\n",
            "with prob: -1.0689076215028763, generated: 真厉害啊\n",
            "\n",
            "with prob: -1.0689076215028763, generated: 真厉害啊\n",
            "\n",
            "with prob: -1.0733989408860605, generated: ！！！！\n",
            "\n",
            "with prob: -1.0733989408860605, generated: ！！！！\n",
            "\n",
            "with prob: -1.0733989408860605, generated: ！！！！\n",
            "\n",
            "with prob: -1.076537773013115, generated: 你在干什么啊！！！！\n",
            "\n",
            "with prob: -1.100884173065424, generated: 太秀了\n",
            "\n",
            "with prob: -1.100884173065424, generated: 太秀了\n",
            "\n",
            "with prob: -1.1051248836198024, generated: 也太秀了吧\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1059347838163376, generated: 强无敌\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1315118968486786, generated: nb\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.1562744565308094, generated: 强啊\n",
            "\n",
            "with prob: -1.178102567182997, generated: 这么强的么！！！！！！！\n",
            "\n",
            "with prob: -1.1842887504026294, generated: ？？？？\n",
            "\n",
            "with prob: -1.1894820317626, generated: 真强啊\n",
            "\n",
            "with prob: -1.2014950341156994, generated: 阿夸nb\n",
            "\n",
            "with prob: -1.2080941319465637, generated: 牛逼！\n",
            "\n",
            "with prob: -1.2246596088888408, generated: yamero！\n",
            "\n",
            "with prob: -1.2643063441462195, generated: 6000066666666\n",
            "\n",
            "with prob: -1.2670717984437943, generated: 牛逼\n",
            "\n",
            "with prob: -1.2670717984437943, generated: 牛逼\n",
            "\n",
            "with prob: -1.2836510886903851, generated: fbk\n",
            "\n",
            "with prob: -1.2842334530183248, generated: 石锤！！！\n",
            "\n",
            "with prob: -1.2922552563250065, generated: 小狐狸\n",
            "\n",
            "with prob: -1.2922552563250065, generated: 小狐狸\n",
            "\n",
            "with prob: -1.2922552563250065, generated: 小狐狸\n",
            "\n",
            "with prob: -1.2922552563250065, generated: 小狐狸\n",
            "\n",
            "with prob: -1.2959117829799651, generated: 强无敌！\n",
            "\n",
            "with prob: -1.3017129396709304, generated: xswl\n",
            "\n",
            "with prob: -1.307947039604187, generated: 夏哥！！！\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.309391736984253, generated: 草\n",
            "\n",
            "with prob: -1.3136989582174767, generated: 阿库娅！！！！\n",
            "\n",
            "with prob: -1.3201989121735096, generated: 带制作\n",
            "\n",
            "with prob: -1.3201989121735096, generated: 带制作\n",
            "\n",
            "with prob: -1.342495026687781, generated: 阿夸真强\n",
            "\n",
            "with prob: -1.3509205602042909, generated: 阿夸nb！\n",
            "\n",
            "with prob: -1.3668327987194062, generated: 强啊！\n",
            "\n",
            "with prob: -1.3786243870854378, generated: 厉害啊\n",
            "\n",
            "with prob: -1.393360633024713, generated: tql（滑稽）\n",
            "\n",
            "with prob: -1.3955556023865938, generated: 祭妹！！！！\n",
            "\n",
            "with prob: -1.4046373396995477, generated: 很强的（滑稽）\n",
            "\n",
            "with prob: -1.4080644255736843, generated: mooooo！！！！\n",
            "\n",
            "with prob: -1.408432142343372, generated: gkd\n",
            "\n",
            "with prob: -1.408432142343372, generated: gkd\n",
            "\n",
            "with prob: -1.430901788175106, generated: 还行\n",
            "\n",
            "with prob: -1.430901788175106, generated: 还行\n",
            "\n",
            "with prob: -1.437722569597619, generated: 夏哥啊！！！\n",
            "\n",
            "with prob: -1.4585648775100708, generated: 好强啊\n",
            "\n",
            "with prob: -1.4585648775100708, generated: 好强啊\n",
            "\n",
            "with prob: -1.459075024840422, generated: 池帝！！！！！！！！！！！！！！！！\n",
            "\n",
            "with prob: -1.4669642969965935, generated: 真秀啊\n",
            "\n",
            "with prob: -1.471384060382843, generated: 是糖！\n",
            "\n",
            "with prob: -1.4727605491876603, generated: !!!\n",
            "\n",
            "with prob: -1.4793079644441605, generated: 不愧是你\n",
            "\n",
            "with prob: -1.4932436915447138, generated: 阿夸贴贴！\n",
            "\n",
            "with prob: -1.5196190284768818, generated: 夏哥suki\n",
            "\n",
            "with prob: -1.5368215076625347, generated: 我死了\n",
            "\n",
            "with prob: -1.56377707918485, generated: mahamah\n",
            "\n",
            "with prob: -1.5690529346466064, generated: t旋\n",
            "\n",
            "with prob: -1.576883915066719, generated: 高达！\n",
            "\n",
            "with prob: -1.5894752740859985, generated: 我着火了\n",
            "\n",
            "with prob: -1.6133636066690087, generated: （赞赏）\n",
            "\n",
            "with prob: -1.6159366911107844, generated: 有能猜出来的小狐狸！！！\n",
            "\n",
            "with prob: -1.6426020748913288, generated: 引起不适\n",
            "\n",
            "with prob: -1.6426020748913288, generated: 引起不适\n",
            "\n",
            "with prob: -1.6502119749784498, generated: quaitibe\n",
            "\n",
            "with prob: -1.659213438630104, generated: 夏哥！\n",
            "\n",
            "with prob: -1.6626648157835007, generated: 夸哥\n",
            "\n",
            "with prob: -1.6746386364102364, generated: 秀啊\n",
            "\n",
            "with prob: -1.6768085624401767, generated: 夏哥？！！！！！\n",
            "\n",
            "with prob: -1.6808625968794029, generated: 都很强啊\n",
            "\n",
            "with prob: -1.6823573460181553, generated: 万岁！！！\n",
            "\n",
            "with prob: -1.7050397736951708, generated: ...\n",
            "\n",
            "with prob: -1.7054539397358894, generated: 牛批\n",
            "\n",
            "with prob: -1.7054539397358894, generated: 牛批\n",
            "\n",
            "with prob: -1.7108550710448374, generated: 夸（指正）\n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7155606746673584, generated: \n",
            "\n",
            "with prob: -1.7276328454415004, generated: 好厉害啊！\n",
            "\n",
            "with prob: -1.7317735314369203, generated: 真香！\n",
            "\n",
            "with prob: -1.7356712836772203, generated: 哈哈哈哈哈\n",
            "\n",
            "with prob: -1.7356712836772203, generated: 哈哈哈哈哈\n",
            "\n",
            "with prob: -1.7474539053160698, generated: 既视感\n",
            "\n",
            "with prob: -1.757422645886739, generated: 玩家\n",
            "\n",
            "with prob: -1.757422645886739, generated: 玩家\n",
            "\n",
            "with prob: -1.7712759375572205, generated: 啊！\n",
            "\n",
            "with prob: -1.7740772639711697, generated: 开挂了吧\n",
            "\n",
            "with prob: -1.7758328795433045, generated: nb（\n",
            "\n",
            "with prob: -1.7799551554955542, generated: 挺好的\n",
            "\n",
            "with prob: -1.7808231748640537, generated: 我酸了\n",
            "\n",
            "with prob: -1.7808231748640537, generated: 我酸了\n",
            "\n",
            "with prob: -1.7818906754255295, generated: 老爱\n",
            "\n",
            "with prob: -1.7898799379666646, generated: 什么都能做到\n",
            "\n",
            "with prob: -1.7945636610190074, generated: 真秀啊！\n",
            "\n",
            "with prob: -1.7966268241405488, generated: nbi\n",
            "\n",
            "with prob: -1.7985799133777618, generated: wai\n",
            "\n",
            "with prob: -1.8097313717007637, generated: 死宅\n",
            "\n",
            "with prob: -1.8097313717007637, generated: 死宅\n",
            "\n",
            "with prob: -1.830656852573199, generated: ！！\n",
            "\n",
            "with prob: -1.8444862000644207, generated: 我哭了\n",
            "\n",
            "with prob: -1.8450186848640442, generated: 真tq\n",
            "\n",
            "with prob: -1.8491852186620235, generated: 太长了\n",
            "\n",
            "with prob: -1.8537599325180054, generated: 好棒！\n",
            "\n",
            "with prob: -1.8860824838280679, generated: 太甜了\n",
            "\n",
            "with prob: -1.890383964870125, generated: 绝了\n",
            "\n",
            "with prob: -1.8951162338256835, generated: 辣条！\n",
            "\n",
            "with prob: -1.895920800311225, generated: 四保一夸哥\n",
            "\n",
            "with prob: -1.9030663184821606, generated: 爱酱牛逼\n",
            "\n",
            "with prob: -1.9056298285722733, generated: cbk\n",
            "\n",
            "with prob: -1.9304261704285939, generated: 给力啊！\n",
            "\n",
            "with prob: -1.9352923120771135, generated: 也太强啦吧\n",
            "\n",
            "with prob: -1.9407006320543587, generated: 个锤子啊！（棒读）\n",
            "\n",
            "with prob: -1.9409337490797043, generated: 上手\n",
            "\n",
            "with prob: -1.9429865591228008, generated: 我单推夏哥！\n",
            "\n",
            "with prob: -1.966102653316089, generated: 你在干什么阿！\n",
            "\n",
            "with prob: -1.9746907711029054, generated: 屑屑狐狸\n",
            "\n",
            "with prob: -1.9807280227541924, generated: 雅蠛蝶！\n",
            "\n",
            "with prob: -1.9817636013031006, generated: ！\n",
            "\n",
            "with prob: -1.9817636013031006, generated: ！\n",
            "\n",
            "with prob: -2.0041205352172256, generated: 阿夸倒下了\n",
            "\n",
            "with prob: -2.0141668803989887, generated: 是真的强啊，夏哥\n",
            "\n",
            "with prob: -2.0143794134259223, generated: 真牛逼\n",
            "\n",
            "with prob: -2.0166279872258506, generated: 夏哥\n",
            "\n",
            "with prob: -2.022284323722124, generated: 故意的屑\n",
            "\n",
            "with prob: -2.0518899142742155, generated: nbb\n",
            "\n",
            "with prob: -2.0601429156959057, generated: 运气选手\n",
            "\n",
            "with prob: -2.068361833691597, generated: 天下第一！\n",
            "\n",
            "with prob: -2.0684827466805777, generated: quax\n",
            "\n",
            "with prob: -2.0730758786201475, generated: 真的强运\n",
            "\n",
            "with prob: -2.0756435245275497, generated: 吧！\n",
            "\n",
            "with prob: -2.0790997594594955, generated: 无敌力\n",
            "\n",
            "with prob: -2.0812692809849977, generated: 都是神仙vtb\n",
            "\n",
            "with prob: -2.103866675592144, generated: 都能胰岛素下啊kora！\n",
            "\n",
            "with prob: -2.1304210821787515, generated: 石锤，草\n",
            "\n",
            "with prob: -2.154435109347105, generated: ，你在干什么！（\n",
            "\n",
            "with prob: -2.1549484729766846, generated: 啊\n",
            "\n",
            "with prob: -2.1647062953561544, generated: 玩家湊阿夸\n",
            "\n",
            "with prob: -2.1663560292550494, generated: 强无敌，带制作\n",
            "\n",
            "with prob: -2.1701821287473044, generated: 卧槽\n",
            "\n",
            "with prob: -2.1701821287473044, generated: 卧槽\n",
            "\n",
            "with prob: -2.1723278679086695, generated: 玩家水平（笑）\n",
            "\n",
            "with prob: -2.1779485805725147, generated: 终于知道自己做到是自己的形状了（悲）\n",
            "\n",
            "with prob: -2.17823688685894, generated: 很强\n",
            "\n",
            "with prob: -2.180716320872307, generated: 四天王\n",
            "\n",
            "with prob: -2.1884278994984925, generated: 过于草\n",
            "\n",
            "with prob: -2.1884875267744066, generated: hso\n",
            "\n",
            "with prob: -2.1896118447184563, generated: 反杀\n",
            "\n",
            "with prob: -2.212165791541338, generated: 吧。。\n",
            "\n",
            "with prob: -2.2254920909181237, generated: (^＿ω)o\n",
            "\n",
            "with prob: -2.2529038802410164, generated: ：我好了\n",
            "\n",
            "with prob: -2.2586198571053417, generated: 什么鬼，我要看夸哥拜年\n",
            "\n",
            "with prob: -2.2688712775707245, generated: 啊喂\n",
            "\n",
            "with prob: -2.2689420204036557, generated: （×）（nb）感谢（√）\n",
            "\n",
            "with prob: -2.2696490830608775, generated: 可以挡屏幕的了\n",
            "\n",
            "with prob: -2.2803988218307496, generated: 你在干嘛\n",
            "\n",
            "with prob: -2.2817727737128735, generated: 神仙还行\n",
            "\n",
            "with prob: -2.2859550025314093, generated: 太帅了\n",
            "\n",
            "with prob: -2.3341374595959983, generated: 吧\n",
            "\n",
            "with prob: -2.334581304873739, generated: 代打www\n",
            "\n",
            "with prob: -2.33673345297575, generated: 最喜欢的屑\n",
            "\n",
            "with prob: -2.3389869928359985, generated: 天才\n",
            "\n",
            "with prob: -2.3578646034002304, generated: 不好的\n",
            "\n",
            "with prob: -2.3748025553567067, generated: 5000s\n",
            "\n",
            "with prob: -2.378439238667488, generated: 真·狐狸\n",
            "\n",
            "with prob: -2.3938505355268718, generated: 邪恶的屑\n",
            "\n",
            "with prob: -2.3944144900888205, generated: 算了!!!\n",
            "\n",
            "with prob: -2.405132565115179, generated: 场力！！！\n",
            "\n",
            "with prob: -2.411204256117344, generated: 的房间（大嘘）\n",
            "\n",
            "with prob: -2.418281240388751, generated: 爽了\n",
            "\n",
            "with prob: -2.4191165566444397, generated: 红石\n",
            "\n",
            "with prob: -2.4267096668481827, generated: 都可以上功\n",
            "\n",
            "with prob: -2.430341104666392, generated: 人人\n",
            "\n",
            "with prob: -2.4388913065195084, generated: 双杀\n",
            "\n",
            "with prob: -2.439093366265297, generated: 算了吧！\n",
            "\n",
            "with prob: -2.442828523615996, generated: ksub\n",
            "\n",
            "with prob: -2.4473713606595995, generated: 玩家带师\n",
            "\n",
            "with prob: -2.449152601261934, generated: 太甜了8\n",
            "\n",
            "with prob: -2.453723672777457, generated: 高速咏唱\n",
            "\n",
            "with prob: -2.4549308816591897, generated: 都是在放水\n",
            "\n",
            "with prob: -2.455140581727028, generated: 是阿夸\n",
            "\n",
            "with prob: -2.4874407052993774, generated: 惊人\n",
            "\n",
            "with prob: -2.4921841740608217, generated: 的梗力\n",
            "\n",
            "with prob: -2.5023128151893617, generated: cg！\n",
            "\n",
            "with prob: -2.5216812628010907, generated: 霸权？？？\n",
            "\n",
            "with prob: -2.527972361445427, generated: 真漂亮哦\n",
            "\n",
            "with prob: -2.538877233862877, generated: 带狐狸\n",
            "\n",
            "with prob: -2.566239913304647, generated: 厉害\n",
            "\n",
            "with prob: -2.5929402618535926, generated: 满雨！！！\n",
            "\n",
            "with prob: -2.6071400973014534, generated: 弹幕（指正）\n",
            "\n",
            "with prob: -2.6119066221373424, generated: 我最喜欢的设定\n",
            "\n",
            "with prob: -2.6143140367099216, generated: 夸神上班！！\n",
            "\n",
            "with prob: -2.618739446004232, generated: 运气\n",
            "\n",
            "with prob: -2.626619517803192, generated: 铝朋友\n",
            "\n",
            "with prob: -2.6283523821168475, generated: 有能人头的白给！\n",
            "\n",
            "with prob: -2.6408627298660576, generated: 也是胡萝卜（确信）\n",
            "\n",
            "with prob: -2.6417023433372835, generated: 节奏。。\n",
            "\n",
            "with prob: -2.6455770283937454, generated: 飞起\n",
            "\n",
            "with prob: -2.648955510722266, generated: ，我什么都会掉下去呢\n",
            "\n",
            "with prob: -2.666061990495239, generated: 手柄和fbk\n",
            "\n",
            "with prob: -2.666528736551603, generated: 厉害了我死\n",
            "\n",
            "with prob: -2.669966538747152, generated: 秀\n",
            "\n",
            "with prob: -2.6704305609067283, generated: 了\n",
            "\n",
            "with prob: -2.686375002066294, generated: 复活\n",
            "\n",
            "with prob: -2.69523446900504, generated: 泪，流了天下\n",
            "\n",
            "with prob: -2.695953242480755, generated: 秀啊哈哈哈哈\n",
            "\n",
            "with prob: -2.699715037519733, generated: 夏哥就是你夸\n",
            "\n",
            "with prob: -2.7056861945561, generated: 你们都有神仙啊\n",
            "\n",
            "with prob: -2.708275705575943, generated: 杰洛特\n",
            "\n",
            "with prob: -2.720747563242912, generated: 前面的秀\n",
            "\n",
            "with prob: -2.7263656556606293, generated: 中文的表情\n",
            "\n",
            "with prob: -2.756450906395912, generated: 大佬还行\n",
            "\n",
            "with prob: -2.7654250065485635, generated: 奇迹\n",
            "\n",
            "with prob: -2.785317789763212, generated: vtb玩家时间\n",
            "\n",
            "with prob: -2.7888389341533184, generated: 无敌房\n",
            "\n",
            "with prob: -2.8126924570117677, generated: 什么时候做到地啊\n",
            "\n",
            "with prob: -2.8193436674773693, generated: 抠现场\n",
            "\n",
            "with prob: -2.8208163965493442, generated: ：你就是诗音（错乱）\n",
            "\n",
            "with prob: -2.833661139011383, generated: 开着刀里\n",
            "\n",
            "with prob: -2.8376409241131375, generated: 带着着对比就行了\n",
            "\n",
            "with prob: -2.84745717048645, generated: 失败\n",
            "\n",
            "with prob: -2.8488348004492847, generated: 还行，都不会投自己的原因\n",
            "\n",
            "with prob: -2.857631151378155, generated: 都会控制的意思(悲)\n",
            "\n",
            "with prob: -2.884815672878176, generated: 404服666\n",
            "\n",
            "with prob: -2.8889182967444262, generated: 意外的拼呢\n",
            "\n",
            "with prob: -2.909475225955248, generated: 炮歌\n",
            "\n",
            "with prob: -2.9106537302335105, generated: 爷爷\n",
            "\n",
            "with prob: -2.912304633430072, generated: 是不是都漏了出来\n",
            "\n",
            "with prob: -2.9167429519196353, generated: 放水了，夏哥\n",
            "\n",
            "with prob: -2.920275926589966, generated: 海王啊\n",
            "\n",
            "with prob: -2.9247468411922455, generated: 强妹\n",
            "\n",
            "with prob: -2.926620751619339, generated: 最强\n",
            "\n",
            "with prob: -2.9335090344150863, generated: 阿夸好强，草这真是个神仙\n",
            "\n",
            "with prob: -2.945263276497523, generated: 50w个\n",
            "\n",
            "with prob: -2.960847592353821, generated: 很有自知之明\n",
            "\n",
            "with prob: -2.9812241047620773, generated: 警觉！\n",
            "\n",
            "with prob: -2.9849919537082314, generated: 佛了\n",
            "\n",
            "with prob: -2.9875664873556658, generated: 真就很麻烦啊真的太秀了\n",
            "\n",
            "with prob: -2.988042503595352, generated: 卡兹\n",
            "\n",
            "with prob: -2.9881696129838624, generated: 加一金？\n",
            "\n",
            "with prob: -2.9938489214650224, generated: 差点都fbk\n",
            "\n",
            "with prob: -2.995178847014904, generated: 流向的屑\n",
            "\n",
            "with prob: -3.0022485447116196, generated: 玩家妨碍出现了阿夸\n",
            "\n",
            "with prob: -3.0105397339378084, generated: 夏哥患者阿夸附体\n",
            "\n",
            "with prob: -3.0105924010276794, generated: 撒撒啦\n",
            "\n",
            "with prob: -3.030961387687259, generated: 还能自己倒的不错啊\n",
            "\n",
            "with prob: -3.0329839129533087, generated: 为什么带着帽子？\n",
            "\n",
            "with prob: -3.038265922665596, generated: 完美白给\n",
            "\n",
            "with prob: -3.0385983188947043, generated: 路过\n",
            "\n",
            "with prob: -3.0423828760782876, generated: 剪辑\n",
            "\n",
            "with prob: -3.0554002724587916, generated: 5分钟草\n",
            "\n",
            "with prob: -3.056970328092575, generated: 复刻\n",
            "\n",
            "with prob: -3.0570663483813405, generated: 开车了\n",
            "\n",
            "with prob: -3.0701017677783966, generated: ，，谢谢！\n",
            "\n",
            "with prob: -3.10201872587204, generated: 更加夸了\n",
            "\n",
            "with prob: -3.122274198383093, generated: 无敌的王者\n",
            "\n",
            "with prob: -3.127320726712545, generated: 舒适\n",
            "\n",
            "with prob: -3.1374621093273163, generated: 危\n",
            "\n",
            "with prob: -3.147996289655566, generated: 骚夸\n",
            "\n",
            "with prob: -3.1636565368622542, generated: 惊人+1\n",
            "\n",
            "with prob: -3.166142016649246, generated: 这个节奏牛逼\n",
            "\n",
            "with prob: -3.180709257721901, generated: 夏哥终于在笑\n",
            "\n",
            "with prob: -3.1854954063892365, generated: 带不到\n",
            "\n",
            "with prob: -3.2110976025462152, generated: 岂不是能\n",
            "\n",
            "with prob: -3.21722149848938, generated: 爆破\n",
            "\n",
            "with prob: -3.2341566930214563, generated: 有能东西的問（错乱\n",
            "\n",
            "with prob: -3.2537981271743774, generated: 带锅\n",
            "\n",
            "with prob: -3.2542189558347068, generated: 欢喜\n",
            "\n",
            "with prob: -3.2617868036031723, generated: 惊人时光\n",
            "\n",
            "with prob: -3.263757586479187, generated: 学学！\n",
            "\n",
            "with prob: -3.2703777899344764, generated: 两圈玩家ww\n",
            "\n",
            "with prob: -3.2744559049606323, generated: 炮模式！\n",
            "\n",
            "with prob: -3.2788159042596816, generated: 在b站，屑单狐狸的\n",
            "\n",
            "with prob: -3.2822953263918557, generated: 真甜(指正\n",
            "\n",
            "with prob: -3.2834844584576786, generated: vtb全部稳了\n",
            "\n",
            "with prob: -3.286440336704254, generated: 强无敌赛\n",
            "\n",
            "with prob: -3.287811689078808, generated: 果然是队引擎\n",
            "\n",
            "with prob: -3.295868791639805, generated: 巨秀\n",
            "\n",
            "with prob: -3.313787220045924, generated: 时双背刺，exaie\n",
            "\n",
            "with prob: -3.318399391240544, generated: ：这箭给我吧！\n",
            "\n",
            "with prob: -3.341558429722985, generated: （对表情）不愧是阿夸，有能来迟\n",
            "\n",
            "with prob: -3.342078745365143, generated: 请珍惜\n",
            "\n",
            "with prob: -3.357630491256714, generated: 真小技术\n",
            "\n",
            "with prob: -3.3577584326267242, generated: 色萨！\n",
            "\n",
            "with prob: -3.3669058280065656, generated: 以前本身玩过小狐狸\n",
            "\n",
            "with prob: -3.3698737770318985, generated: 总不一样啊\n",
            "\n",
            "with prob: -3.370213569700718, generated: 的姿势，wdl赛高\n",
            "\n",
            "with prob: -3.390477533638477, generated: 真的强走\n",
            "\n",
            "with prob: -3.403998696378299, generated: 不带圈团啊\n",
            "\n",
            "with prob: -3.4053075313568115, generated: 无敌qa\n",
            "\n",
            "with prob: -3.421983480453491, generated: 脏话\n",
            "\n",
            "with prob: -3.435969332853953, generated: 好吃\n",
            "\n",
            "with prob: -3.44354529120028, generated: 丢人哈哈哈\n",
            "\n",
            "with prob: -3.450368736471449, generated: 我养你啊舔\n",
            "\n",
            "with prob: -3.4531658478081226, generated: 准备支援\n",
            "\n",
            "with prob: -3.460264839231968, generated: 黄金屑\n",
            "\n",
            "with prob: -3.4696882565816245, generated: 亏了\n",
            "\n",
            "with prob: -3.4704431742429733, generated: 再拆吧！\n",
            "\n",
            "with prob: -3.4708945266902447, generated: 要被中炮了\n",
            "\n",
            "with prob: -3.4719696611166, generated: 什么建筑？\n",
            "\n",
            "with prob: -3.48490449488163, generated: 的大傻\n",
            "\n",
            "with prob: -3.496003913382689, generated: 开带文字吧\n",
            "\n",
            "with prob: -3.511638800303141, generated: 可惜\n",
            "\n",
            "with prob: -3.517654371261597, generated: 带你都行\n",
            "\n",
            "with prob: -3.551947130076587, generated: 能做到你死下去的屑（\n",
            "\n",
            "with prob: -3.5662001570065818, generated: 不是龙骑吗\n",
            "\n",
            "with prob: -3.5839081673766486, generated: 像房间（无慈悲）\n",
            "\n",
            "with prob: -3.5943731665611267, generated: 夏哥一败涂地\n",
            "\n",
            "with prob: -3.601303456351161, generated: 暴走印\n",
            "\n",
            "with prob: -3.6050362028181553, generated: 老年人规避\n",
            "\n",
            "with prob: -3.6180315734818578, generated: 接金币nb\n",
            "\n",
            "with prob: -3.627758564054966, generated: 直播的海王\n",
            "\n",
            "with prob: -3.629920873377058, generated: gg，放弃是真的服\n",
            "\n",
            "with prob: -3.6380126029253006, generated: 开场引起\n",
            "\n",
            "with prob: -3.6498516178689897, generated: 啥时候带道具东西的女人\n",
            "\n",
            "with prob: -3.7045843567699195, generated: 锤子姿势草\n",
            "\n",
            "with prob: -3.734688378870487, generated: 炮社会\n",
            "\n",
            "with prob: -3.735120313614607, generated: 好怀念哈哈哈哈哈哈\n",
            "\n",
            "with prob: -3.744114634063509, generated: 能不能看夏哥公司印的\n",
            "\n",
            "with prob: -3.7585023939609528, generated: 紫屏\n",
            "\n",
            "with prob: -3.768248677253723, generated: 哎哟\n",
            "\n",
            "with prob: -3.7874586624758586, generated: 发动里你的合集\n",
            "\n",
            "with prob: -3.804519772529602, generated: 明抢\n",
            "\n",
            "with prob: -3.8164995374778905, generated: 屑^ku\n",
            "\n",
            "with prob: -3.829224849740664, generated: 可以拿头烂\n",
            "\n",
            "with prob: -3.859658984201295, generated: 直接匹配一下船吧\n",
            "\n",
            "with prob: -3.8685433665911355, generated: 上狐狸和狐狸\n",
            "\n",
            "with prob: -3.8809940993785856, generated: 沙旗！\n",
            "\n",
            "with prob: -3.8856784656643866, generated: 吧哦呜呜\n",
            "\n",
            "with prob: -3.8863864739735923, generated: 律师\n",
            "\n",
            "with prob: -3.888514479994774, generated: 敌人看弹幕\n",
            "\n",
            "with prob: -3.8929854115205154, generated: 挡着草无脑现场\n",
            "\n",
            "with prob: -3.8948094376495908, generated: ：那不是鸡蛋甲\n",
            "\n",
            "with prob: -3.896884521469474, generated: 赫萝找到了\n",
            "\n",
            "with prob: -3.923299476504326, generated: 所有人啊\n",
            "\n",
            "with prob: -3.934663224965334, generated: 野生夸奖\n",
            "\n",
            "with prob: -3.93774339184165, generated: 看来你还能靠进步\n",
            "\n",
            "with prob: -3.9467629194259644, generated: 医疗糖\n",
            "\n",
            "with prob: -3.947859287261963, generated: 5倍速\n",
            "\n",
            "with prob: -3.951401801407337, generated: 当和bv玩家，理解理解\n",
            "\n",
            "with prob: -3.971073974456106, generated: 打个高速的难度\n",
            "\n",
            "with prob: -3.976213589310646, generated: 鸡腿对决\n",
            "\n",
            "with prob: -4.00739162415266, generated: 监督包\n",
            "\n",
            "with prob: -4.009818986058235, generated: 鬼才o\n",
            "\n",
            "with prob: -4.011326938867569, generated: 厉害型\n",
            "\n",
            "with prob: -4.01564185321331, generated: 你就还记得康啊，我本身都知道都会开车\n",
            "\n",
            "with prob: -4.016498416662216, generated: 放开狐狸\n",
            "\n",
            "with prob: -4.019300810992718, generated: 来新哪个dd怎么样的鉴\n",
            "\n",
            "with prob: -4.045615854859352, generated: 玩家时食用\n",
            "\n",
            "with prob: -4.045680029317737, generated: 全体加班\n",
            "\n",
            "with prob: -4.053082704544067, generated: 养帽子\n",
            "\n",
            "with prob: -4.126668363809586, generated: 拆人的孙\n",
            "\n",
            "with prob: -4.131677071253459, generated: 生活\n",
            "\n",
            "with prob: -4.139269371383956, generated: 华都提醒去打人，你真是让我康康\n",
            "\n",
            "with prob: -4.163731012493372, generated: 浪兴\n",
            "\n",
            "with prob: -4.202024012804031, generated: 不能确定\n",
            "\n",
            "with prob: -4.218044549226761, generated: 非常头还\n",
            "\n",
            "with prob: -4.220351457595825, generated: ＝\n",
            "\n",
            "with prob: -4.310615337320736, generated: 这时候丢就出来了\n",
            "\n",
            "with prob: -4.328374475240707, generated: 直接放眼镜更\n",
            "\n",
            "with prob: -4.3386496137827635, generated: 不如小爱\n",
            "\n",
            "with prob: -4.35120655596257, generated: 烤肉man叔叔\n",
            "\n",
            "with prob: -4.403208217273156, generated: 三角魂可以了\n",
            "\n",
            "with prob: -4.425700229406357, generated: 手游怨\n",
            "\n",
            "with prob: -4.454935265438897, generated: 都头上开高达\n",
            "\n",
            "with prob: -4.464483526349068, generated: 车万激光\n",
            "\n",
            "with prob: -4.539321660995483, generated: 两个模式而\n",
            "\n",
            "with prob: -4.5509622891743975, generated: 关注点\n",
            "\n",
            "with prob: -4.562028998339718, generated: 可以去和小雷电玩涅..\n",
            "\n",
            "with prob: -4.568703254063924, generated: 眼里\n",
            "\n",
            "with prob: -4.580348481734593, generated: 脏话现场荣，是个惊人的高速级别\n",
            "\n",
            "with prob: -4.622054657340049, generated: 航母ア玩家\n",
            "\n",
            "with prob: -4.6353764198720455, generated: 最速大片\n",
            "\n",
            "with prob: -4.66601977199316, generated: 无敌堵法\n",
            "\n",
            "with prob: -4.692158451676368, generated: 紫口雨\n",
            "\n",
            "with prob: -4.715335745364428, generated: 代价撒在↑\n",
            "\n",
            "with prob: -4.754016323635976, generated: 事件，夏哥没学会，自己失望的难受\n",
            "\n",
            "with prob: -4.8255777433514595, generated: 洗伤害真特殊\n",
            "\n",
            "with prob: -4.865840668028051, generated: 个死字的第二个在家是真强\n",
            "\n",
            "with prob: -4.974194836802781, generated: 躲挡e姥爷的地方\n",
            "\n",
            "with prob: -4.999598628116979, generated: ，佐仓阳盲人双杀\n",
            "\n",
            "with prob: -5.047751107386181, generated: 上你火东京湾！\n",
            "\n",
            "with prob: -5.0581814758479595, generated: 黄金分身\n",
            "\n",
            "with prob: -5.072842905918757, generated: 别再上去怨\n",
            "\n",
            "with prob: -5.088983294243614, generated: 乌拉，蜜汁引擎\n",
            "\n",
            "with prob: -5.105748725788934, generated: ：那还回来聽\n",
            "\n",
            "with prob: -5.443265177309513, generated: 复活系列门男子草\n",
            "\n",
            "with prob: -5.539427265524864, generated: 平舞呀新表情\n",
            "\n",
            "with prob: -5.6780622661113735, generated: =哈洁\n",
            "\n",
            "with prob: -5.879504108428955, generated: 令一些哈哈\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWsSJRL4-4Eb",
        "colab_type": "text"
      },
      "source": [
        "####Get the generated data & origin data for comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaWYpM7u-2W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "origin_whole_list = []\n",
        "\n",
        "'get some origin data in a list'\n",
        "with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "    data = json.load(json_file, encoding='UTF-8')\n",
        "\n",
        "'randomly sample some data from the list'\n",
        "index_range = np.arange(len(data))\n",
        "np.random.shuffle(index_range)\n",
        "\n",
        "'obtain the data'\n",
        "for index in range(PREDICT_LEN):\n",
        "  origin_data = ''.join(data[index_range[index]])\n",
        "  origin_whole_list.append(origin_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBZ5easAlr8",
        "colab_type": "code",
        "outputId": "856f1c61-7455-4098-d3fb-2d64e1ced47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"real_message samples: {}\".format(origin_whole_list[:5]))\n",
        "print(\"fake_messages samples: {}\".format(generated_whole_list[:5]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "real_message samples: ['真实', '只有红茶可以吗？', '。。。', '草。', '啊乌拉拉']\n",
            "fake_messages samples: [[-4.001725305403982, '没有，不能再脱，，去光的大叔吧\\n'], [-2.4308136055866876, '尖叫\\n'], [-3.0729534327983856, '什么玩意\\n'], [-2.739387810230255, '强。\\n'], [-5.275522403419018, '疼论\\n']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKzuCc9LA6vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'with pandas data frame'\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "df = pd.DataFrame(columns=['message', 'label'])\n",
        "for origin in origin_whole_list:\n",
        "  df = df.append({'message': origin, 'label': 1}, ignore_index=True)\n",
        "for fake in generated_whole_list:\n",
        "  df = df.append({'message': fake, 'label': 0}, ignore_index=True)\n",
        "df = shuffle(df)\n",
        "df.to_csv(\"/content/fake_danmaku_evaluation.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2a5cGsSTEBQD"
      },
      "source": [
        "## What's next\n",
        "\n",
        "* Danmaku-caption: generate danmaku based on context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL73nQwIO5yO",
        "colab_type": "text"
      },
      "source": [
        "### For TS-Javascript\n",
        "Load and save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPIYk5gTO3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction_model = lstm_model(seq_len=1,batch_size=8, stateful=True)\n",
        "# prediction_model.load_weights('/tmp/bard_{}.h5'.format(0))\n",
        "# prediction_model.save('/tmp/bard.js.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
