# Artificial_dd

[![Generic badge](https://img.shields.io/badge/Tensorflow-keras-<COLOR>.svg)](https://shields.io/) 
[![Generic badge](https://img.shields.io/badge/github-dd_center-<COLOR>.svg)](https://shields.io/)
<p>
    <img src="model_picture/dd_center.png"/>
</p>
### âš ï¸ Notification

Please keep this repo **private**, and please notice that this is **not** an open-source software currently. 

### ğŸ“ƒ Introduction

A software that can send context-based fake danmaku. 

Please write more things here if any of you want to describe this project...

### â˜ï¸ Model structure

This is a sequence-to-sequence model with an attention mechanism. The encoder is used to compress the information in input messages, and the decoder is responsible for generating new texts based on the inputs. Here is the model structure:

<p>
    <img src="model_picture/model.png"/>
</p>

### ğŸš§ Utilization

1. Run the following command, and you should see two folders named 'content' and 'tmp'.
```
bash ./download_sources.sh
```
2. Run the following command, and you should see the outputs.
```
python3 model_process.py
```

### ğŸ‰ Output example

With inputs:
```
ï¼Ÿï¼Ÿï¼Ÿ
è¦è®©è€å¸ˆå°å¿ƒä¸€ç‚¹
å¤ªå¯çˆ±äº†
å‰é¢çš„æ³¥å¢äº†
æ²¡æœ‰è¶…ç¾ä¸½3dï¼Œç™½ç­‰äº†
ï¼Ÿæ²¡è§è¿‡è¿™ç§åŒ…æ³•å‘€
awsl
æ¥äº†æ¥äº†
å°é¢ä¸ºä»€ä¹ˆè¿™ä¹ˆéªš
åŒé—®
2333
ä¸‰æ¬¡å…ƒçš„æœ‹å‹ä¹Ÿä¸èƒ½æ”¾å¼ƒå•Š
awsl
awsl
awsl
```
Here are the outputs:
```
é˜¿ä¼Ÿå‡ºæ¥å—æ­»ï¼
wwwww
wwwwwwwwwwwwwwwwwwwwww
å¯çˆ±
awsl
awsl
awsl
è‰
awsl
awsl
çˆ±é…±å¥½å¯çˆ±
awsl
é˜¿ä¼Ÿå‡ºæ¥å—æ­»
awsl
awsl
awsl
114514
ä½ ä¸è¦è¿‡æ¥å•Š
é˜¿ä¼Ÿæ­»äº†
```

### âš¡ï¸ Quick start

First, create an instance and load the model. 

The program will be more time-consuming if you increase the parameter 'BATCH_SIZE', but you will get better results. So, there is a trade-off here. For instance, under current configuration, it took 6.7 seconds to generate 20 fake danmakus on my computer. We could expect it to be much faster in a GPU.

```python
from model_process import model_process
'Initialize'
mp = model_process(BATCH_SIZE = 500)
'Create a generator, load in the trained model'
mp.prepare_for_generator()
```

Second, get the input data.
```python
'We assume we have the following inputs'
data = input_data().return_example_input_list()
input_data().show_input_data()
```
At this step, you should see the following outputs in the console. That's what the program expects as inputs:
```
read in data:
>> ä»Šæ™šæˆ‘ä¸ç¡äº†ï¼
>> åˆºæ¿€åˆºæ¿€
>> washoi
>> é©¬è‡ªç«‹å”±æ­Œâ€¦â€¦
>> çˆ·ä¸ç¡äº†
>> å¤å“¥æ’­ï¼Œæˆ‘å°±é™ªå¤å“¥ç†¬
>> å¤å“¥æ·±å¤œæ¡£
>> ï½‹ï½‹ï½“ï½‹
>> ç©å…‰æ˜ä¸Šä¸‹çš„å…ˆè¸¢äº†
>> ddä»¬æ¢ç‰Œå­
>> åŠå¤œå”±æ­Œï¼Ÿ
>> ã“ã‚“ã°ã‚“ã‚ã£ã—ã‚…ã„
>> washoi
>> washoi
>> (ï½€ãƒ»Ï‰ãƒ»Â´)ç‰™ç™½
```

Then, whenever the new data is available, feed it into the feed_in_data function. Here we use a for loop to simulate this process:
```python
'Use a loop to iterate the data'
for single_data in data:
    'Every time there is a new message available, feed in the data'
    returned_result = mp.feed_in_data(single_data)
```
Whenever there is enough data, the model will output the fake danmakus to the 'returned_result', which is a list. Otherwise, the list will be empty.
