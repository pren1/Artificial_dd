# Artificial_dd

[![Generic badge](https://img.shields.io/badge/Tensorflow-keras-<COLOR>.svg)](https://shields.io/) 
[![Generic badge](https://img.shields.io/badge/github-dd_center-<COLOR>.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/Beam-search-<COLOR>.svg)](https://shields.io/)
<p>
    <img src="model_picture/dd_center.png"/>
</p>

### âš ï¸ Notification

Please keep this repo **private**, and please notice that this is **not** an open-source software currently. 

### ğŸŒ² Request Packages

[![Generic badge](https://img.shields.io/badge/gdown-orange.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/keras-red.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/tensorflow-blue.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/scipy-blueviolet.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/tqdm-lightgrey.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/jieba-ff69b4.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/flask-success.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/uwsgi-yellow.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/pandas-grey.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/pydot-cyan.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/graphviz-brown.svg)](https://shields.io/)

### ğŸ“ƒ Introduction

A software that can send context-based fake danmaku. 

Please write more things here if any of you want to describe this project...

### ğŸ‰ Demo

We got a [demo video](https://pan.baidu.com/s/18Pkr_VAEnXuME-NMG7HMdQ) here! Yeah!

### â˜ï¸ Model structure

This is a sequence-to-sequence model with the attention mechanism. The encoder is used to compress the information in input messages, and the decoder is responsible for generating new texts based on the inputs. Here is the model structure:

<p>
    <img src="model_picture/model.png"/>
</p>

### âš¡ï¸ Quick start

1. Run the following command, and you should see two folders named 'content' and 'tmp'.
```
bash ./download_sources.sh
```
2. Then, run:
```
uwsgi --http :80 --enable-threads --wsgi-file ./model_process.py --callable app --pyargv "--batch_size=100"
```
Notice that you can always tune the batch_size when you start this service. If you get larger batch_size, then the model performance will increase, but the program would be more time-consuming. So, there is a trade-off here. Please select this parameter based on your machine. Generally, if you have a GPU, set the batch_size to 500 is recommended.

3. After that, you should figure out your room id. Here are some frequently used examples. Note that not all the vtubers are tested, so use this model at your own risk.
```json
{
    "1": "çŒ«å®«æ—¥å‘Official",
    "16": "æ–°ç§‘å¨˜Official",
    "38": "å¤§ç¥æ¾ªOfficial",
    "54": "AIChannelå®˜æ–¹",
    "70": "ç™½ä¸Šå¹é›ªOfficial",
    "96": "ç´«å’²è¯—éŸ³Official",
    "133": "è¼å¤œæœˆOfficial",
    "146": "ç¥å­æ-Official",
    "152": "å…”çº±mimi_Official",
    "191": "ç¥æ¥½ä¸ƒå¥ˆOfficial",
    "224": "é™†å©‰è¹GodRiku",
    "233": "æ¹Š-é˜¿åº“å¨…Official",
    "258": "æœªæ¥æ˜-MiraiAkari",
    "266": "å¤è‰²ç¥­Official",
    "283": "èŒ¯è‹“çŒ«ä¸é»‘",
    "286": "ç¥æ¥½Mea_Official",
    "309": "çŠ¬å±±ç‰å§¬Official",
    "322": "æ³ é¸¢yousa"
}
```
Take a look at the room_id_mapping.json under the content folder for other vtubers.

4. After that, post the following url. You should also specify the room_id this time, and it will boost the performance:
```
http://YOUR_IP:PORT/processjson?
message="è¿·è¿­è¿·è¿­, è¿·è¿­è¿·è¿­å¸•é‡Œæ¡‘, 23333333, å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ"
&room_id=286
&use_beam_search=True
&temperature=1.0
&message_number=40
```
Please feel free to tune the following parameters for better performance:

parameter name | data type | default value | Description |
--- | --- | --- | --- 
message | string | --- | input danmaku message |
room_id | int | --- | the room id of the vtubers |
use_beam_search | bool | False | apply this should boost the performance, but it will also slow the inference process |
temperature | float | 1.0 | **This only works when you use the beam search.** Increase this value above 1.0 will increase the diversity of the generated text and mistakes. In contrast, decrease this value will make the model more confident and more conservative. |
message_number | int | 40 | The total message numbers generated each time |

The python program should respond:
```json
{
    "result": "not enough input messages"
}
```
However, if you send enough inputs, the program will return the generated messages:
```json
{
    "result": [
        "tekokiå‡ºæ¥è¡€å‹æ‹‰æ»¡\n",
        "è¿·è¿­å¸•é‡Œå¸•é‡Œ\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "tekokiè¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è‰ï¼Œè¿·è¿­è¿·è¿­\n",
        "tekokiè¡€å‹upï¼Œç§‹æ¢¨è†ï¼Œè¯·å¤§å¤§ç•™ä¸‹ç•™ä¸‹ä½ ä»¬çš„å…³æ³¨",
        "è¿·è¿­è¿·è¿­paryiè¿·è¿­è¿·è¿­æ¡‘\n",
        "è¡€å‹å‡é«˜æ‹‰æ»¡\n",
        "tekokiè¿·è¿­\n",
        "tekokiè¡€å‹\n",
        "tekokiè¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¡€å‹å‡é«˜æ‹‰æ»¡\n",
        "tekokiè¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¿·è¿­è¿·è¿­çˆ¬çŠæ¡‘ï¼ˆtekokiï¼‰\n",
        "è¡€å‹æ‹‰æ»¡\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "tekokiè¿·è¿­è¡€å‹\n",
        "è‰\n",
        "è¿·è¿­è¿·è¿­çˆ¬çŠæ¡‘ï¼ˆtekokiï¼‰\n",
        "è¿·è¿­è¿·è¿­çˆ¬çŠæ¡‘ï¼Œè¡€å‹ä¸Šå‡ï¼ˆï¼‰è¡€å‹çˆ†ç‚¸\n",
        "è‰\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¿·è¿­è¿·è¿­å¸•é‡Œæ¡‘  \n",
        "tekokiå‡ºé“å§\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è¿·è¿­è¿·è¿­paryiæ¡‘\n",
        "è‰\n",
        "è‰\n",
        "è¿·è¿­è¿·è¿­\n",
        "tekokiå‡ºæ¥è¡€å‹æ‹‰æ»¡\n",
        "è‰ï¼Œè¿™æ®µ\n",
        "è¿·è¿­è¿·è¿­\n",
        "tekokiè¿·è¿­\n",
        "è‰\n",
        "tekokiè¡€å‹æ‹‰æ»¡\n",
        "è¿·è¿­è¿·è¿­å¸•é‡Œæ¡‘  \n",
        "è¡€å‹çˆ†ç‚¸\n",
        "è¡€å‹æ‹‰æ»¡\n"
    ]
}
```
