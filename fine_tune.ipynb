{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "fine_tune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6ZDpd9XzFeN"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "KUu4vOt5zI9d",
        "colab": {}
      },
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "edfbxDDh2AEs"
      },
      "source": [
        "## Predict vtuber danmaku with Cloud TPUs and Keras\n",
        "#### Modified from \"Predict Shakespeare with Cloud TPUs and Keras\"\n",
        "Author github ID: pren1, coco401, simon3000, Afanyiyu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RNo1Vfghpa8j"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a Cloud TPU. This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
        "\n",
        "The model trains for 10 epochs and completes in approximately 1 hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QrprJD-R-410"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_I0RdnOSkNmi"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Train on TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kYxeFuKCUx9d"
      },
      "source": [
        "TPUs are located in Google Cloud, for optimal performance, they read data directly from Google Cloud Storage (GCS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lvo0t7XVIkWZ"
      },
      "source": [
        "## Data, model, and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xzpUtDMqmA-x"
      },
      "source": [
        "In this file, you train the model utilizing the danmaku data shown below:\n",
        "\n",
        "<blockquote>\n",
        "[\"完事\",\"了\",\"这\",\"是\",\"？\"],\n",
        "\n",
        "[\"来\",\"了\"],\n",
        "\n",
        "[\"哇\",\"我\",\"刚\",\"忙\",\"完\",\"o\",\"r\",\"z\"]\n",
        "</blockquote>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USsmqZM9n4T_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KRQ6Fjra3Ruq"
      },
      "source": [
        "### Download data\n",
        "You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpdTZVIYXaR",
        "colab_type": "code",
        "outputId": "0fcaadbe-d428-4b41-b287-ca263312cf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1QWBjb9vk8TZhc9tZqxV1-BbVTj0GlMBy\n",
        "# !gdown https://drive.google.com/uc?id=1i6JH7x7SsAFYYX_EU1z5DHr1YQKeVyzN\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1bRf5YnXh8dkLwqgz4IdqaxRMKmzq0pxI\n",
        "# !gdown https://drive.google.com/uc?id=1jEo1ObjoHqI0JuPsCQuRndxw6xIPxjoR\n",
        "# !gdown https://drive.google.com/uc?id=1tBO5Bxfu3FRLLuudIQ_xHvi0t6LfO34m\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1DEVIMMeCLqtsiOKA3TgX2KRSDdvjhuYr\n",
        "# !gdown https://drive.google.com/uc?id=1T5OpFmiT00MFZYNHyGFXqpEcLvdIBVOr\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=169jYxkPev2lkfMy8eu497EuukLxXcMF-\n",
        "\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1V5juWnxQXwOOxJarxJ0V8-nVPF87QshX\n",
        "# !gdown https://drive.google.com/uc?id=1B0UaIeixggEg30SUw3uvWxDGbabJo9NV\n",
        "# !gdown https://drive.google.com/uc?id=1QLl2kqsPDoWhbmM22N9bdt1gOsuLRAdv\n",
        "\n",
        "'128'\n",
        "# !gdown https://drive.google.com/uc?id=12taxnPvsqgaQuJDW9d1Pqzsz7ykR5rub\n",
        "# !gdown https://drive.google.com/uc?id=1HuJT-GXvgZUs8turPoKP2tEvZJEc_uNH\n",
        "# !gdown https://drive.google.com/uc?id=1GHG5S-LiEI-hCp47hxt6whZVbgqv3V8k\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1CPRLIVGlCwG2OshG1Ix5PGFO6z2G8pzc\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=11oycRZUgPN3eFgQy_ZvADUwv9IzUEt0g\n",
        "# !gdown https://drive.google.com/uc?id=1wRVWnrJJPXz4E6I8x2sIoxlOFdWu1uMJ\n",
        "\n",
        "\n",
        "'512'\n",
        "# !gdown https://drive.google.com/uc?id=1yzco4NHi7pCb9RLCT_rPWzv-SxjRvvrN\n",
        "# !gdown https://drive.google.com/uc?id=11hhsqbAvSC-qxo1eoiV2jUqDK2Ta-fuB\n",
        "# !gdown https://drive.google.com/uc?id=1AOtc7nFGPr34YoS7uUkmSovVOfSenKMF\n",
        "\n",
        "'512 new'\n",
        "!gdown https://drive.google.com/uc?id=1zEy1FI8IJJNPqbF_u4fw1LYb6LRJFcR8\n",
        "!gdown https://drive.google.com/uc?id=11KWv0drUpoEcJto6lkrw3QJRxWFXsxdK\n",
        "!gdown https://drive.google.com/uc?id=1BNho7u9E3bpRnIUrkJzyqFxHg1xgtFCW"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zEy1FI8IJJNPqbF_u4fw1LYb6LRJFcR8\n",
            "To: /content/glove-512.npy\n",
            "17.2MB [00:00, 79.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11KWv0drUpoEcJto6lkrw3QJRxWFXsxdK\n",
            "To: /content/pure_live_512.json\n",
            "489MB [00:06, 75.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BNho7u9E3bpRnIUrkJzyqFxHg1xgtFCW\n",
            "To: /content/glove-512-words.pkl\n",
            "100% 127k/127k [00:00<00:00, 37.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXFaG8qtvfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "9a7cbc2d-9ef9-4bc1-e8e2-ac5f23105ac2"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1FFSNsBeFlevU8v3xfQCUvDCBNSuP52PL"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FFSNsBeFlevU8v3xfQCUvDCBNSuP52PL\n",
            "To: /content/room_id_mapping.json\n",
            "\r  0% 0.00/13.9k [00:00<?, ?B/s]\r100% 13.9k/13.9k [00:00<00:00, 5.59MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15pAi9ShXdU",
        "colab_type": "text"
      },
      "source": [
        "## Process the data with index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmWOiUB4hV24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "24fe9e56-8d4a-4e83-b9ea-1061042168f1"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pdb\n",
        "import collections\n",
        "import distutils\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from keras.utils import plot_model\n",
        "\n",
        "def remove_rare_characters(obtain_freq, min_times):\n",
        "    'Remove characters occur less than min_times'\n",
        "    freq_characters = []\n",
        "    rare_characters = []\n",
        "    print(\"processing character dictionary...\")\n",
        "    for i in tqdm(range(len(obtain_freq))):\n",
        "        char, freq_num = obtain_freq[i]\n",
        "        if freq_num > min_times:\n",
        "            freq_characters.append(char)\n",
        "        else:\n",
        "            rare_characters.append(char)\n",
        "    return freq_characters, rare_characters\n",
        "\n",
        "def process_data_with_index(txt, minimum_occur_time):\n",
        "    text = (open(txt).read())\n",
        "    text = text.lower()\n",
        "    characters = sorted(list(set(text)), reverse=True)\n",
        "    'at here, remove character occurs less than minimum_occur_time'\n",
        "    obtain_freq = collections.Counter(text).most_common()\n",
        "    freq_characters, rare_characters = remove_rare_characters(obtain_freq, minimum_occur_time)\n",
        "    print(\"freq char: {}\\{}, rare char: {}\\{}\".format(len(freq_characters), len(characters), len(rare_characters), len(characters)))\n",
        "    return freq_characters, rare_characters, text\n",
        "\n",
        "# 'the character should occur this much time if they wanna to be taken into account'\n",
        "# minimum_occur_time = 100\n",
        "context_vector_length = 100\n",
        "context_seq_length = 130\n",
        "batch_size = 2048\n",
        "# SHAKESPEARE_TXT = '/content/bert-master_danmaku_text_pure.txt'\n",
        "# 'Use the following path to just save you some time'\n",
        "# preprocessed_TXT = '/content/rectified_input.txt'\n",
        "# characters, rare_characters, input_text = process_data_with_index(SHAKESPEARE_TXT, minimum_occur_time)\n",
        "\n",
        "'load in characters, and embedding matrix'\n",
        "with open('/content/glove-512-words.pkl', 'rb') as f:\n",
        "    characters = pickle.load(f)\n",
        "    'also add end part, and beginning part'\n",
        "    '口呆口'\n",
        "    characters[-1] = 'eos'\n",
        "    'magnet'\n",
        "    characters[-2] = '\\n'\n",
        "# preprocessed_TXT = '/content/new_filtered_data.json'\n",
        "preprocessed_TXT = '/content/pure_live_512.json'\n",
        "# preprocessed_TXT = '/content/fbk_fine_tune.json'\n",
        "embedding_matrix = np.load('/content/glove-512.npy')\n",
        "'show something about embedding'\n",
        "\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "\n",
        "def transform(txt):\n",
        "    return np.asarray([char_to_n[c] for c in txt], dtype=np.int32)\n",
        "\n",
        "# def remove_unkown_character_from_text(txt, rare_characters):\n",
        "#     'Remove char in rare_characters from txt' \n",
        "#     for x in tqdm(rare_characters):\n",
        "#         try:\n",
        "#             txt = txt.replace(x, \"\")\n",
        "#         except ValueError:\n",
        "#             pass\n",
        "#     return txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3V4V-Jxmuv3",
        "colab": {}
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "import json\n",
        "def input_fn(seq_len=context_seq_length, batch_size=batch_size):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "    data = json.load(json_file, encoding='UTF-8')\n",
        "    # data = data[:100000]\n",
        "    'process the data'\n",
        "    txt = []\n",
        "    label_part = []\n",
        "    for single_meg in tqdm(data):\n",
        "      single_meg[0].insert(0, 'eos')\n",
        "      single_meg[0].append('\\n')\n",
        "      label = [single_meg[1]] * len(single_meg[0])\n",
        "      txt.extend(single_meg[0])\n",
        "      label_part.extend(label)\n",
        "    # occur_index =[i for i in range(len(txt)) if txt[i] in ['口呆口', 'magnet']]\n",
        "    \n",
        "    new_txt = []\n",
        "    new_label_part = []\n",
        "    for (single, label) in tqdm(zip(txt, label_part)):\n",
        "      if single not in  ['口呆口', 'magnet']:\n",
        "        new_txt.append(single)\n",
        "        new_label_part.append(label)\n",
        "    txt = tf.constant(transform(new_txt), dtype=tf.int32)\n",
        "    label_part = tf.constant(new_label_part, dtype=tf.int32)\n",
        "    print(\"Processing the txt: {}, with label: {}\".format(txt[1000:1020], label_part[1000:1020]))\n",
        "    'If the input is preprocessed_TXT, then you do not need this one'\n",
        "    # txt = remove_unkown_character_from_text(txt, rare_characters)\n",
        "  # txt = np.asarray(txt)\n",
        "  # label_part = np.asarray(label_part)\n",
        "  # source = tf.constant(transform(res), dtype=tf.int32)\n",
        "  \n",
        "  # def generator():\n",
        "  #   for txt_sig, label_sig in zip(txt, label_part):\n",
        "  #     yield txt_sig, label_sig\n",
        "  \n",
        "  # ds = tf.data.Dataset.from_generator(generator, output_types=(tf.string, tf.int32)).batch(seq_len+1, drop_remainder=True)\n",
        "  import time\n",
        "  start_time = time.time()\n",
        "  ds = tf.data.Dataset.from_tensor_slices((txt, label_part)).batch(seq_len+1, drop_remainder=True)\n",
        "  print(\"--- slice tensor spends: %s seconds ---\" % (time.time() - start_time))\n",
        "  # ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
        "\n",
        "  def split_input_target(chunk, label_chunk):\n",
        "    context_vector = chunk[:context_vector_length]\n",
        "    input_text = chunk[context_vector_length:-1]\n",
        "    target_text = chunk[context_vector_length+1:]\n",
        "    'Simply use the first element as the chunk label'\n",
        "    label_value = label_chunk[:1]\n",
        "    return (context_vector, input_text, label_value), target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "  return ds.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bbb05dNynDrQ"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
        "\n",
        "The input dimension to the Embedding layer is the same as our vocabulary size.\n",
        "\n",
        "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLEM-fLJlEEt",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "HALF_EMBEDDING_DIM = int(EMBEDDING_DIM/2)\n",
        "regularizer_coefficient = 0.000001\n",
        "def lstm_model(seq_len=30, context_length = context_vector_length, batch_size=None, stateful=True):\n",
        "    \"\"\"Language model: Encoder decoder favor for context term\"\"\"\n",
        "    room_id_bit =  tf.keras.Input(name='room_id_bit', shape=(1,), batch_size=batch_size, dtype=tf.int32)\n",
        "    one_hot_embedding_id = tf.keras.backend.one_hot(room_id_bit, num_classes = EMBEDDING_DIM)\n",
        "\n",
        "    encoder_input = tf.keras.Input(name='Encoder_input', shape=(context_length,), batch_size=batch_size, dtype=tf.int32)\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim=len(characters), output_dim=EMBEDDING_DIM, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)\n",
        "    encode_embedding = embedding_layer(encoder_input)\n",
        "    'Then, we concatenate the embedding to provide more info'\n",
        "    rich_info_embedding = tf.keras.layers.concatenate([encode_embedding, one_hot_embedding_id],axis=1)\n",
        "\n",
        "    enc_lstm1, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HALF_EMBEDDING_DIM, name='encoder_lstm_1', return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))(rich_info_embedding)\n",
        "    state_h_1 = tf.keras.layers.concatenate([forward_h, backward_h])\n",
        "    state_c_1 = tf.keras.layers.concatenate([forward_c, backward_c])\n",
        "    enc_lstm1 = tf.keras.layers.Dropout(0.6)(enc_lstm1)\n",
        "    encoder_states_1 = [state_h_1, state_c_1]\n",
        "\n",
        "    enc_lstm2, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HALF_EMBEDDING_DIM, name='encoder_lstm_2', return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))(enc_lstm1)\n",
        "    \n",
        "    'Double concatenate'\n",
        "    enc_lstm2 = tf.keras.layers.concatenate([enc_lstm2, one_hot_embedding_id],axis=1)\n",
        "\n",
        "    state_h_2 = tf.keras.layers.concatenate([forward_h, backward_h])\n",
        "    state_c_2 = tf.keras.layers.concatenate([forward_c, backward_c])\n",
        "    encoder_states_2 = [state_h_2, state_c_2]\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = tf.keras.Input(name='Decoder_input', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    lstm_1_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_1', stateful=stateful, return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient))\n",
        "    lstm_1, _, _ = lstm_1_layer(decode_embedding, initial_state=encoder_states_1)\n",
        "    dropout_lstm_1 = tf.keras.layers.Dropout(0.6)(lstm_1)\n",
        "    lstm_2_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_2', stateful=stateful, return_state=True, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient))\n",
        "    lstm_2, _, _  = lstm_2_layer(dropout_lstm_1, initial_state=encoder_states_2)\n",
        "    dropout_lstm_2 = tf.keras.layers.Dropout(0.6)(lstm_2)\n",
        "\n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([dropout_lstm_2, enc_lstm2])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, enc_lstm2])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, dropout_lstm_2])\n",
        "    dense_layer_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(EMBEDDING_DIM*4, activation='tanh' , kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))\n",
        "    predicted_char_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(characters), activation='softmax' , kernel_regularizer=tf.keras.regularizers.l2(regularizer_coefficient)))\n",
        "\n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    predicted_char = predicted_char_layer(dense_layer_output_1)\n",
        "    \n",
        "    Model = tf.keras.Model(inputs=[encoder_input, decoder_inputs, room_id_bit], outputs=[predicted_char])\n",
        "    # Model.summary()\n",
        "    tf.keras.utils.plot_model(Model, show_shapes=True, to_file='model.png')\n",
        "\n",
        "    'For reference, also prepared some tricks'\n",
        "    encoder_model = tf.keras.Model([encoder_input, room_id_bit], [encoder_states_1[0], encoder_states_1[1], encoder_states_2[0], encoder_states_2[1], enc_lstm2])\n",
        "    tf.keras.utils.plot_model(encoder_model, show_shapes=True, to_file='encoder_model.png')\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_h1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    \n",
        "    'Add 2 for double room_id_inputs'\n",
        "    encoder_output_in = tf.keras.Input(shape=(context_vector_length + 2, EMBEDDING_DIM,))\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, decoder_state_input_h1, decoder_state_input_c1]\n",
        "    d_o, state_h, state_c = lstm_1_layer(decode_embedding, initial_state=decoder_states_inputs[:2])\n",
        "    d_o, state_h1, state_c1 = lstm_2_layer(d_o, initial_state=decoder_states_inputs[-2:])\n",
        "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
        "\n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([d_o, encoder_output_in])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, encoder_output_in])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, d_o])\n",
        "\n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    decoder_outputs = predicted_char_layer(dense_layer_output_1)\n",
        "    decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs + [encoder_output_in], [decoder_outputs] + decoder_states)\n",
        "    tf.keras.utils.plot_model(decoder_model, show_shapes=True, to_file='decoder_model.png')\n",
        "\n",
        "    return Model, encoder_model, decoder_model\n",
        "\n",
        "def get_stand_alone_decoder(seq_len=30, context_length = context_vector_length, batch_size=None, stateful=True):\n",
        "    decoder_state_input_h = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_h1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    decoder_state_input_c1 = tf.keras.Input(shape=(EMBEDDING_DIM,))\n",
        "    \n",
        "    decoder_inputs = tf.keras.Input(name='Decoder_input', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "    \n",
        "    'Add 2 for room_id_inputs'\n",
        "    encoder_output_in = tf.keras.Input(shape=(context_vector_length + 2, EMBEDDING_DIM,))\n",
        "\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim=len(characters), output_dim=EMBEDDING_DIM, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)\n",
        "    decode_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, decoder_state_input_h1, decoder_state_input_c1]\n",
        "    lstm_1_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_1', stateful=stateful, return_state=True, return_sequences=True)\n",
        "    d_o, state_h, state_c = lstm_1_layer(decode_embedding, initial_state=decoder_states_inputs[:2])\n",
        "    lstm_2_layer = tf.keras.layers.LSTM(EMBEDDING_DIM, name='decoder_lstm_2', stateful=stateful, return_state=True, return_sequences=True)\n",
        "    d_o, state_h1, state_c1 = lstm_2_layer(d_o, initial_state=decoder_states_inputs[-2:])\n",
        "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
        "    \n",
        "    'try to add attention here~'\n",
        "    attention = tf.keras.layers.Dot(axes=[2, 2])([d_o, encoder_output_in])\n",
        "    attention = tf.keras.layers.Activation('softmax', name='attention')(attention)\n",
        "    context = tf.keras.layers.Dot(axes=[2, 1])([attention, encoder_output_in])\n",
        "    decoder_combined_context = tf.keras.layers.concatenate([context, d_o])\n",
        "    \n",
        "    dense_layer_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(EMBEDDING_DIM*4, activation='tanh'))\n",
        "    predicted_char_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(characters), activation='softmax'))\n",
        "    \n",
        "    dense_layer_output_1 = dense_layer_1(decoder_combined_context)\n",
        "    decoder_outputs = predicted_char_layer(dense_layer_output_1)\n",
        "    decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs + [encoder_output_in], [decoder_outputs] + decoder_states)\n",
        "    tf.keras.utils.plot_model(decoder_model, show_shapes=True, to_file='decoder_model.png')\n",
        "    return decoder_model\n",
        "\n",
        "def step_decay(epoch):\n",
        "    import math\n",
        "    initial_lrate = 0.002\n",
        "    drop = 0.6\n",
        "    epochs_drop = 1.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
        "    print(\"lrate: {}, epoch: {}\".format(lrate, epoch))\n",
        "    return lrate\n",
        "\n",
        "# training_model,encoder_model, decoder_model = lstm_model(seq_len=30, context_length = context_vector_length, stateful=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VzBYDJI0_Tfm"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "First, we need to create a distribution strategy that can use the TPU. In this case it is TPUStrategy. You can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods `fit`, `evaluate` and `predict` use the TPU.\n",
        "\n",
        "Again note that we train with `stateful=False` because while training, we only care about one batch at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExQ922tfzSGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "d144569d-9f7f-46d6-e3b8-fcc726e802b8"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  training_model,encoder_model, decoder_model = lstm_model(seq_len=30, context_length = context_vector_length, stateful=False)\n",
        "  lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "  adam = tf.keras.optimizers.RMSprop(lr=0.0, decay=0.0)\n",
        "  # 'layer frozen'\n",
        "  # for layer in training_model.layers[:-1]:\n",
        "\t#   layer.trainable = False\n",
        "  training_model.compile(\n",
        "      # optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      optimizer = 'adam',\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Initializing the TPU system: 10.101.65.226:8470\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.101.65.226:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 704029884542538213)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11809702863081524530)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7134459078828150512)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3099481297865549167)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11013269248056982738)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6699578043650939033)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6068997911194841390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2271043448218633134)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12645556493431268163)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12348844042209948309)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16241003888196586395)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKIYptTySorJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training_model.load_weights('/tmp/bard_{}.h5'.format(0))\n",
        "  # 'load pretrained weights'\n",
        "# encoder_model.load_weights('encoder.h5')\n",
        "# decoder_model.load_weights('decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec2YZwjojEye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ab12f83-69be-4694-c2a6-fdb1f5f408e1"
      },
      "source": [
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=30000,\n",
        "    epochs=1,\n",
        "    callbacks=[lrate]\n",
        ")\n",
        "saver_index = 0\n",
        "training_model.save_weights('/tmp/bard_{}.h5'.format(saver_index), overwrite=True)\n",
        "#training_model.save('/tmp/bard_{}.js.h5'.format(saver_index), overwrite=True)\n",
        "saver_index += 100"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9585006/9585006 [00:09<00:00, 981351.88it/s]\n",
            "54743715it [00:21, 2540997.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing the txt: Tensor(\"strided_slice:0\", shape=(20,), dtype=int32), with label: Tensor(\"strided_slice_1:0\", shape=(20,), dtype=int32)\n",
            "--- slice tensor spends: 0.004544734954833984 seconds ---\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "lrate: 0.0024, epoch: 0\n",
            "30000/30000 [==============================] - 7849s 262ms/step - loss: 2.4226 - sparse_categorical_accuracy: 0.5896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV3mrS_xjDgZ",
        "colab_type": "text"
      },
      "source": [
        "# 删除EOS 和\\n  &  分割 语句"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfXgJWGTjBd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_EOS(input: list) -> list:\n",
        "    while 'eos' in input:\n",
        "        input.remove('eos')\n",
        "    str1 = \"\".join(input)\n",
        "    res = str1.split('\\n')\n",
        "    del res[-1]\n",
        "    for single in res:\n",
        "      print(single)\n",
        "    return res  \n",
        "# print(delete_EOS(['eos', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '观看', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '转播man', '\\n', 'eos', '8', '8', '8', '8', '8', '8', '8', '\\n', 'eos', '感谢', '转播man', '\\n', 'eos', 'a', 'n', 't', 'i', '路过', '，', '不用', '管', '\\n', 'eos', 'o', 'k', 'k', '\\n', 'eos', 'a', 'n', 't', 'i', '需要', '理由', '吗', '？', '\\n']\n",
        "# ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwiZBc6dmIzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'get some real text inputs'\n",
        "import random\n",
        "import copy\n",
        "# preprocessed_TXT = '/content/new_filtered_data.json'\n",
        "def load_in_texts():\n",
        "  with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "      data = json.load(json_file, encoding='UTF-8')\n",
        "      'process the data'\n",
        "      txt = []\n",
        "      label_part = []\n",
        "      for single_meg in tqdm(data):\n",
        "        single_meg[0].insert(0, 'eos')\n",
        "        single_meg[0].append('\\n')\n",
        "        label = [single_meg[1]] * len(single_meg[0])\n",
        "        txt.extend(single_meg[0])\n",
        "        label_part.extend(label)\n",
        "\n",
        "      'remove that does not belongs to characters...'\n",
        "      new_txt = []\n",
        "      new_label_part = []\n",
        "      for (single, label) in tqdm(zip(txt, label_part)):\n",
        "        if single not in  ['口呆口', 'magnet']:\n",
        "          new_txt.append(single)\n",
        "          new_label_part.append(label)\n",
        "      print(\"updated txt, remove from {} to {}, examples: {}\".format(len(txt), len(new_txt), new_txt[:20]))\n",
        "      return new_txt, new_label_part"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQ7ymktLXek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder_model.save_weights('/content/drive/My Drive/encoder.h5')\n",
        "# decoder_model.save_weights('/content/drive/My Drive/decoder.h5')\n",
        "\n",
        "encoder_model.save_weights('/tmp/encoder.h5')\n",
        "decoder_model.save_weights('/tmp/decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TCBtcpZkykSf"
      },
      "source": [
        "### Make predictions with the model\n",
        "\n",
        "Use the trained model to make predictions and generate your own fake danmaku messages.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
        "\n",
        "The predictions are done on the CPU so the batch size (5) in this case does not have to be divisible by 8.\n",
        "\n",
        "Note that when we are doing predictions or, to be more precise, text generation, we set `stateful=True` so that the model's state is kept between batches. If stateful is false, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
        "\n",
        "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"草\" and the output probabilities are \"草\" (0.65), \"哈\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"草\" and \"哈\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCzuRE6lLzUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "20e599a1-066b-4b07-aa18-1477a46a1570"
      },
      "source": [
        "!pip install pprint"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pprint\n",
            "  Downloading https://files.pythonhosted.org/packages/99/12/b6383259ef85c2b942ab9135f322c0dce83fdca8600d87122d2b0181451f/pprint-0.1.tar.gz\n",
            "Building wheels for collected packages: pprint\n",
            "  Building wheel for pprint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pprint: filename=pprint-0.1-cp36-none-any.whl size=1250 sha256=5f487790c519b1b83cc04194193cae729bfa52d1ab8602f2f707f870bc7148f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/d4/c6/16a6495aecc1bda5d5857bd036efd50617789ba9bea4a05124\n",
            "Successfully built pprint\n",
            "Installing collected packages: pprint\n",
            "Successfully installed pprint-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tU7M-EGGxR3E",
        "colab": {}
      },
      "source": [
        "# BATCH_SIZE = 2\n",
        "# PREDICT_LEN = 30\n",
        "# BEAM_SIZE = 10\n",
        "# import pprint\n",
        "# # Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# # We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# # time and predicting the next character.\n",
        "# # tf.keras.backend.clear_session()\n",
        "\n",
        "# _, encoder_model, _ = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# # encoder_model.load_weights('/content/drive/My Drive/encoder.h5')\n",
        "# encoder_model.load_weights('/tmp/encoder.h5')\n",
        "\n",
        "# decoder_model = get_stand_alone_decoder(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# # decoder_model.load_weights('/content/drive/My Drive/decoder.h5')\n",
        "# decoder_model.load_weights('/tmp/decoder.h5')\n",
        "\n",
        "# # We seed the model with our initial string, copied BATCH_SIZE times\n",
        "# # seed_txt = ['了', '\\n', 'eos', '再次', '坑', '乌拉', '\\n', 'eos', 'p', 'o', 'i', '和', '乌拉', '拉', '在', '一起', '打', '吗', '？', '\\n', 'eos', '马赛克', '！', '\\n', 'eos', '噗', '\\n', 'eos', 'e', 'm', 'm', 'm', '火车', '晚', '了', 'p', 'o', 'i', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '\\n', 'eos', '你', '要', '知道', '我', '写', '过', '文', '\\n', 'eos', '咬', '滑稽', '\\n', 'eos', '这', '是', '一段', '有', '画面', '的', '文字', '\\n', 'eos', '污', '拉拉', '\\n', 'eos', '乌拉', '拉', '晚上', '好', '\\n', 'eos', '去', '拿', '对面', '油', '\\n', 'eos', '晚', '好', '吖', '\\n', 'eos', '哇', '，', '乌拉', '粉丝', '还有', '看', '高', '达', '的', '\\n', 'eos', '（', '6', '9', '）', '\\n', 'eos', '我', '怀疑', '你', '在', '开车', '\\n', 'eos', '尝试', '理解', 'p', 'o', 'i', '\\n', 'eos', '好多', '.', '。', '。', '。', '\\n', 'eos', '不要', '火车', 'p', 'o', 'i', '？', '\\n', 'eos', '上次', '。', '。', '。', '\\n', 'eos', '我', '看看', '我', '下', '个', 'c', 'o', 'h', '\\n', 'eos', '穿', '模', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '？', '\\n', 'eos', '天狗', '\\n']\n",
        "# # seed_txt = ['eos','天', '狗','\\n']*200\n",
        "\n",
        "# print(\"Load in texts...\")\n",
        "# seed = transform(load_in_texts(100))\n",
        "# # print(seed_txt)\n",
        "\n",
        "# # seed = transform(load_in_texts(100))\n",
        "# seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "# # Encode the input as state vectors.\n",
        "# state_and_output = encoder_model.predict(seed)\n",
        "# states_value = [state_and_output[:4]] * BEAM_SIZE\n",
        "# encoder_output = state_and_output[-1]\n",
        "\n",
        "# # Solve decoder things\n",
        "# last_predictions = [np.array([[7010]]*BATCH_SIZE, dtype=np.int32)]\n",
        "# # Beam serach impl!\n",
        "# 'at first, all prob is 0'\n",
        "# path_saver = [[0, list()]] * BEAM_SIZE\n",
        "# print(\"Preforming beam search...\")\n",
        "# for i in tqdm(range(PREDICT_LEN)):\n",
        "#   total_slot = []\n",
        "#   for beam_words_id in range(len(last_predictions)):\n",
        "#     'for this words'\n",
        "#     last_word = last_predictions[beam_words_id]\n",
        "#     next_probits, h, c, h1, c1 = decoder_model.predict([last_word] + states_value[beam_words_id] + [encoder_output])\n",
        "#     'assign right states value'\n",
        "#     if len(last_predictions) == 1:\n",
        "#       'at the beginning, just renew all the state values'\n",
        "#       for i in range(BEAM_SIZE):\n",
        "#         states_value[i] = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "#     else:\n",
        "#       'if we have more choices, only update one'\n",
        "#       states_value[beam_words_id] = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "#     batch_id = 0\n",
        "#     next_probits = next_probits[:, 0, :][batch_id]\n",
        "#     'for each batch'\n",
        "#     'just a dirty work around, since all batch return the same results'\n",
        "#     previous_prob = path_saver[beam_words_id]\n",
        "#     top_k_words = next_probits.argsort()[-BEAM_SIZE:]\n",
        "#     for words_id in top_k_words:\n",
        "#       total_slot.append([previous_prob[0] + np.log(next_probits[words_id]), previous_prob[1] + [words_id]])\n",
        "#   'sort by the first prob'\n",
        "#   path_saver = sorted(total_slot, key=lambda tup:tup[0])[-BEAM_SIZE:]\n",
        "#   last_predictions = []\n",
        "  \n",
        "#   'Do something to get last predictions work here'\n",
        "#   for previous_path_tuple in path_saver:\n",
        "#     last_path = previous_path_tuple[1][-1]\n",
        "#     last_predictions.append(np.array([[last_path]]*BATCH_SIZE, dtype=np.int32))\n",
        "\n",
        "# 'generate top k sentences'\n",
        "# fin_res = []\n",
        "# for single_path in path_saver:\n",
        "#   prob = single_path[0]\n",
        "#   sentence = []\n",
        "#   for val in single_path[1]:\n",
        "#     current_char = n_to_char[val]\n",
        "#     sentence.append(current_char)\n",
        "#     if current_char == '\\n':\n",
        "#       break\n",
        "#   generated_sentence = ''.join(sentence)  # Convert back to text\n",
        "#   fin_res.append([prob, generated_sentence])\n",
        "\n",
        "# fin_res = sorted(fin_res, key=lambda tup:tup[0])\n",
        "# pprint.pprint(fin_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9GdlJWwpaFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "PREDICT_LEN = 15\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "_, encoder_model, _ = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# prediction_model.load_weights('/tmp/bard_{}.h5'.format(0))\n",
        "encoder_model.load_weights('/tmp/encoder.h5')\n",
        "\n",
        "decoder_model = get_stand_alone_decoder(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "decoder_model.load_weights('/tmp/decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etpePQVKpdRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54f9fcf1-f7f5-4e27-c37c-142367c964e4"
      },
      "source": [
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "# seed_txt = ['那', '是', '你', '心态', '不行', '\\n', 'eos', '我', '爱', '酱', '真是', '越来越', '聪明', '啦', '？', '\\n', 'eos', '本子', '预定', '\\n', 'eos', '被', '吓', '到', '了', '\\n', 'eos', '哈哈哈', '哈哈哈', '\\n', 'eos', '吹', '\\n', 'eos', '字幕', '没错', '好', '吧', '\\n', 'eos', '代', '打', '当然', '是', '开玩笑', '，', '但是', '说', '多', '了', '是', '真的', '烦', '\\n', 'eos', '代', '打', '是', '不过', '分', '，', '但', '你', '一直', '刷', '，', '你', '不', '烦', '别人', '烦', '\\n', 'eos', '整个', '游戏', '就', '在', '这儿', '卡', '关', '了', '不', '知道', '可以', '跳', '2', '3', '3', '3', '\\n', 'eos', '1', '7', '秒', '\\n', 'eos', 'w', 'w', 'w', 'w']\n",
        "# seed_txt = ['嘛', '（', '清楚', '多', '意', '）', '\\n', 'eos', '完事', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', '很', '懂', '\\n', 'eos', '自己', '都', '笑', '了', '\\n', 'eos', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '\\n', 'eos', '老鸨', '\\n', 'eos', '很', '懂', '。', '。', '。', '我', '第一次', '知道', '\\n', 'eos', 'j', 'k', '一周', '目', '草', '生', '\\n', 'eos', '过于', '清楚', '\\n', 'eos', '艾', '琳', '太', '真实', '了', '\\n', 'eos', '实在', '是', '过于', '清楚', '\\n', 'eos', '怎么', '知道', '的']\n",
        "# seed = transform(seed_txt)\n",
        "new_txt, new_label = load_in_texts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9585006/9585006 [00:09<00:00, 1014864.21it/s]\n",
            "54743715it [00:21, 2532675.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "updated txt, remove from 54743715 to 54743051, examples: ['eos', '这', '游戏', '还', '这么', '暖', '的', '么', '\\n', 'eos', '知道', '太多', '对', '你', '没', '好处', '（', '我', '也', '不']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lRIBiYY5_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a08ccc28-a3b2-4ccb-c268-8b420d00dbe3"
      },
      "source": [
        "len(n_to_char)\n",
        "print(f\"8407: \\n\")\n",
        "print(f\"8408: {n_to_char[8408]}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8407: \n",
            "\n",
            "8408: eos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsU6mY624dad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_text(txt_length, new_txt, new_label, start_pos_type = 0):\n",
        "    proposed_start_index = new_label.index(start_pos_type)\n",
        "    'Currently, we only support 355 vtubers'\n",
        "    proposed_end_index = new_label.index(min(start_pos_type+1, 355))\n",
        "    start_index = random.randint(proposed_start_index, proposed_end_index - txt_length)\n",
        "    clipped_txt_for_test = new_txt[start_index:start_index + txt_length]\n",
        "    clipped_labels_for_test = new_label[start_index:start_index + txt_length]\n",
        "    delete_EOS(copy.deepcopy(clipped_txt_for_test))\n",
        "    return clipped_txt_for_test, clipped_labels_for_test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_xfdSECt4yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "b115e23d-4b08-488e-e25f-f9f242c966a6"
      },
      "source": [
        "room_id_mapping = '/content/room_id_mapping.json'\n",
        "with open(room_id_mapping, encoding='UTF-8') as json_file:\n",
        "      id_mapping_dict = json.load(json_file, encoding='UTF-8')\n",
        "print(f\"mapping_id_res: {id_mapping_dict}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mapping_id_res: {'0': '艾因Eine', '1': '猫宫日向Official', '2': '铃音_Official', '3': '无理MURI', '4': '乌拉の帝国Official', '5': '幽灵子辰', '6': '玉兔精果酱sama', '7': '白音小雪Official', '8': '轻又', '9': 'なるはやちゃん成早酱', '10': 'メアレスOfficial', '11': '金丝蓝染Channel', '12': '佐仓•M•沙耶加', '13': '綾奈奈奈', '14': '惺忪喵Xingsongmiao', '15': '子彧_official', '16': '新科娘Official', '17': '凛星RinStar', '18': '呆梓爱你嗷', '19': '琉绮Ruki', '20': '熊井塔塔子_Channel', '21': '普通人搬运组', '22': '柠狐柃Channel', '23': '夜霧Yogiri', '24': '月霖Official', '25': '莉姬女王', '26': 'nanaha_akahoshi', '27': '夜空梅露Official', '28': '米莎ミサOfficial', '29': '内兹米_Official', '30': 'Milky_Vtuber', '31': '七濑胡桃menherachan', '32': 'Unknown problem', '33': 'Elaine_夜空Project', '34': '静凛字幕组', '35': 'Unknown problem', '36': '烈火火Official', '37': '比西子official', '38': '大神澪Official', '39': 'ENA에나', '40': '御伽莉鹿-官方', '41': '千草はな', '42': '白咲べる搬运', '43': 'Riztech', '44': '银河爱丽丝搬运', '45': '白百合リリィOfficial', '46': '神无月の诗羽一kenlos', '47': '白银诺艾尔Official', '48': '來夢めると搬运组', '49': '路人豆腐铺', '50': '澪木芙格Official', '51': '琥珀-虚颜', '52': '虚拟女友Yomemi', '53': '御伽原江良Official', '54': 'AIChannel官方', '55': '佐久間ねむ搬运', '56': 'tanoki苒佚', '57': '喵枫にゃぁ', '58': '鬼月Channel', '59': 'Unknown problem', '60': '闇ヤミやみ', '61': '安洁Official', '62': 'Ruby_呜哔うび', '63': '贝尔蒙德Official', '64': '饼叽Official', '65': '出雲霞字幕组', '66': '桃桃子Momoko', '67': 'AsPride', '68': '托尔rua_Official', '69': '樱美Yuna-Official', '70': '白上吹雪Official', '71': '花园Serena', '72': '美玲roa搬运组', '73': '小伞一把', '74': '染谷杏', '75': '心斎桥オクトOfficial', '76': '纪游哉', '77': '泡沫こまりOfficial', '78': '屍鬼Channel公式字幕组', '79': '云玉鸾', '80': '江惠惠Official', '81': '草履虫Para', '82': '翎眠眠', '83': '幻天一棵葱Channel', '84': '緋赤エリオ搬运组', '85': '卖火柴的可可亚', '86': '猫叶沅沅Official', '87': '女巫Vivian', '88': '森中花咲Official', '89': '桜吹雪Channel', '90': '狛犬博美Official', '91': 'Creator_Coffee_P', '92': 'fairyschan', '93': '阿德罗伊-埃菲尔德', '94': '狐狐音の前搬运', '95': '白金Platinummm', '96': '紫咲诗音Official', '97': '星宫がんChannel', '98': '幻梦喵Channel', '99': '星星奈_HoshiNa', '100': '润羽露西娅Official', '101': '乐喵Channel', '102': '廉田--Kenlos网咖', '103': '有栖Mana-Official', '104': '木糖纯姐', '105': '水漠豆腐', '106': '鹿乃ちゃん', '107': 'Demeo阿死', '108': '失去梦の咸鱼', '109': 'yakko暗影', '110': '宝钟玛琳Official', '111': '夢乃栞Yumeno_Shiori', '112': '鸠子-JOJOChannel', '113': '狐妖小时莩Channel', '114': '花芽姐妹Official', '115': 'Unknown problem', '116': '小东人魚Official', '117': '古守血遊official', '118': '摩提_MoChi', '119': '涉谷HAL_official', '120': '椎名唯华Official', '121': '天神子兔音Official', '122': 'Leia_夜空Project', '123': '可可洛cocola', '124': '花京院ちえり搬运组', '125': '森永みうofficial', '126': '子狐official', '127': '奈子Nakooo', '128': '菫妃奈Official', '129': 'Twiska', '130': 'dsailab', '131': 'VirtuaReal', '132': '步玎Pudding', '133': '輝夜月Official', '134': '莉泽Official', '135': '樱小夜Saya', '136': '黑夜子Channel', '137': '橘九Official', '138': '8911勘测研究所', '139': '阿修アッシュ', '140': 'Overidea_China', '141': 'VA-Luna', '142': '乙女音Official', '143': '鹰宫莉音Official', '144': '小鸟瑠紫琉Official', '145': 'Unknown problem', '146': '神子杏-Official', '147': '猫竹りせChannel', '148': '大空昴Official', '149': '蒼乃幽姬', '150': '朝比奈蒼', '151': '花町猫耳搬运组', '152': '兔纱mimi_Official', '153': '皆守ひいろOfficial', '154': '花譜的音乐搬运站', '155': '某南', '156': '鶹鸝紧急改名', '157': '鬼畜千寻炎哈组', '158': '淺羽Asaha_Mino', '159': '雫るる_Official', '160': '鈴谷アキ字幕组', '161': '虚拟次元计划', '162': '虚拟宫宫Official', '163': '阿睿琳Irene', '164': 'AIofficial', '165': '舞元启介消防队', '166': '癒月巧可Official', '167': '鈴木骨兔Channel', '168': '夢月ましろ', '169': 'AEF_Channel', '170': '西木木木栗Official', '171': '乔治亚娜钱德勒', '172': 'Unknown problem', '173': '闇夜乃莫露露搬运组', '174': '千鈴めい搬运组', '175': '叶Official', '176': '樋口枫Official', '177': '季毅Jiyi', '178': '富士葵中国粉丝团', '179': '中单光一', '180': 'Kenlos-米莉安めか', '181': '月之宫可依-kenlos偶像', '182': '兽耳放送', '183': '兰若_Ruo', '184': '陆桃Official', '185': '静夜夜-德拉利娜', '186': '魔宵Sakyu_Official', '187': 'しろいぬ', '188': '委员长杂鱼搬运', '189': 'Unknown problem', '190': 'CocoTsuki公式', '191': '神楽七奈Official', '192': '某柯moke', '193': '静凛Official', '194': '萨莎柯里克Official', '195': '海月米田没几个人没组', '196': '山梨Official', '197': '黑川幽梦-Vtuber', '198': '星乃めあOfficial', '199': '萝卜子Official', '200': '绘茉-真空管Dolls', '201': '雨森小夜搬运组', '202': 'Unknown problem', '203': 'dsailab二号机', '204': '爱塔西亚温channel', '205': 'RukiRokiChannel', '206': '帕里_Paryi', '207': '雪樱の神', '208': '竹轮喵Official', '209': '星空悠_Channel', '210': '宇宙莎莎喵', '211': '明纱_Official', '212': 'Ryoko凉子_Official', '213': 'Kanna_康娜Official', '214': '凉宫白樱Official', '215': '凤鸠鸠Channel', '216': 'KMNHZ-China', '217': '阳向心美Official', '218': '辉月兔-Official', '219': '物述有栖Official', '220': '清楚系Vtuber_眠叶绀', '221': '进击的冰糖', '222': 'kinoko杂菌', '223': '爱夏なつ', '224': '陆婉莹GodRiku', '225': '樱巫女Official', '226': 'duduluく', '227': 'Unknown problem', '228': '花丸晴琉Official', '229': 'nyanagooooo', '230': '星ノ谷みやOfficial', '231': '时乃空Official', '232': '游戏部企划official', '233': '湊-阿库娅Official', '234': '兔田佩克拉Official', '235': '竹取かるた字幕组', '236': '本间向日葵Official', '237': '加里Gari', '238': 'Kenlos受付娘-樱喵', '239': '花音酱可怜又可爱', '240': '尼奥蜀黍', '241': '一果Ichigo', '242': '音音继ねね_Official', '243': '芥末说她播了', '244': '绿仙Official', '245': '伊佐夜', '246': '伊樱萤Channel', '247': '日月咪玉-官方', '248': '赤井心Official', '249': '千代家蒲莉莉搬运', '250': 'Siva_小虾鱼_', '251': 'RootChannel', '252': 'Elu的精灵の森', '253': '高槻律official', '254': 'Mr.Quin', '255': '李稍牙牙語', '256': '这是一只YY', '257': '艾麗卡・エリカチャン', '258': '未来明-MiraiAkari', '259': 'DontStop虚拟声优事务所', '260': '能美枫铃だよ', '261': 'Miya_猫窝建设中', '262': 'Ruriko琉璃子Channel', '263': '千洛爱吃肉Chiro', '264': '红晓音Akane', '265': '安奈るり--Kenlos受付娘', '266': '夏色祭Official', '267': '龙胆尊Official', '268': '丹羽加奈--Kenlos店长', '269': '猫又小粥Official', '270': 'AnMr海外转播Channel', '271': '时音Official', '272': '兰若_Re', '273': '原梓--kenlos网咖', '274': '星野由乃Channel', '275': 'Unknown problem', '276': '安堂いなり_official', '277': '混吃等死亚亚子', '278': '清露イクナ搬运组', '279': '响木青Official', '280': '七海Nana7mi', '281': '轴伊Joi_Channel', '282': '宇森ひなこ王国運搬', '283': '茯苓猫不黑', '284': '鹿木博士鹿太Channel', '285': '不知火芙蕾雅Official', '286': '神楽Mea_Official', '287': '转圈再转个圈', '288': '喀秋莎秋明Official', '289': '夕阳莉莉搬运組', '290': '暁ちあき搬运组', '291': '九尾の沙羽Official', '292': '凌依--Kenlos网咖', '293': '璃露Riru', '294': '家长麦Official', '295': '战斗吧歌姬官方账号', '296': '阿A觉得这样也好', '297': '莉莉丝Channel', '298': '亚绮-罗森Official', '299': '织田信姬official', '300': '庞小莹Official', '301': 'MEMproject', '302': '魔女玛露氏Official', '303': 'Unknown problem', '304': 'VTuber作家_艾尔酱搬运', '305': '沙月だよ', '306': '柚唧の微光小剧场', '307': '田中姬铃木雏Official', '308': 'PLIVYou_official', '309': '犬山玉姬Official', '310': '東雲恵Official', '311': '笹木咲Official', '312': '星野_Hoshino', '313': 'hololive', '314': '不能吃的ok酱', '315': '枫言w', '316': '矢泽大凤Channel', '317': 'Yuka碳', '318': '真白くまOfficial', '319': 'Mendako醤Official', '320': '音俣RukaOfficial', '321': '七柠Official', '322': '泠鸢yousa', '323': 'Unknown problem', '324': '雪城真尋搬運組', '325': '十六夜凛子-Kenlos', '326': '海桑Kai_新企划准备中', '327': '早稻叽', '328': '铃果Official', '329': '虚拟DD', '330': 'WINKS事务所', '331': 'ItaChannel', '332': '魔王YALU', '333': 'RiNE_Channel', '334': 'PPHOfficial', '335': '侵蚀公主', '336': '戌神沁音Official', '337': 'AZKi_Official', '338': 'Plutya普露提亚', '339': '樱海棠Official', '340': '莲娜多莲佐Official', '341': '百鬼绫目Official', '342': '本气黑猫', '343': '葛叶Official', '344': '赤星组Channel', '345': '恒天--Kenlos网咖', '346': '朝之姐妹搬运组', '347': '可儿！', '348': '星街彗星Official', '349': '眞白花音_Official', '350': '电脑少女siro_小白', '351': '水科葵搬运', '352': '于川流丹ChildRue', '353': '洛晓曦NICO-channel', '354': '七濑昴Official', '355': '虚拟恶魔Diana'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4ZpIuhNYOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bd46b18-e65c-4b92-b828-7ce4f39c7cae"
      },
      "source": [
        "print(\"Load in texts...\")\n",
        "input_text, input_label = clip_text(100, new_txt, new_label, start_pos_type=70)\n",
        "print(\"the previous text is from: {}\".format(id_mapping_dict[str(input_label)]))\n",
        "seed = transform(input_text)\n",
        "print(\"generating text...\")\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "label_seed = np.repeat(np.expand_dims(input_label, 0), BATCH_SIZE, axis=0)\n",
        "state_and_output = encoder_model.predict((seed, label_seed))\n",
        "states_value = state_and_output[:4]\n",
        "encoder_output = state_and_output[-1]\n",
        "# Solve decoder things, we just happen to have 8408 tokens here \n",
        "predictions = [np.array([[8408]]*BATCH_SIZE, dtype=np.int32)]\n",
        "predictions_prob = []\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits, h, c, h1, c1 = decoder_model.predict([last_word] + states_value + [encoder_output])\n",
        "  next_probits = next_probits[:, 0, :]\n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(len(characters), p=next_probits[i])\n",
        "      # np.argmax(next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  'build the prob case'\n",
        "  prob = []\n",
        "  for batch_id in range(BATCH_SIZE):\n",
        "    prob.append(next_probits[batch_id][next_idx[batch_id]])\n",
        "  predictions_prob.append(np.asarray(prob))\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  # Update states\n",
        "  states_value = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
        "\n",
        "generated_whole_list = []\n",
        "for i in range(BATCH_SIZE):\n",
        "  # print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  p_prob = [predictions_prob[j][i] for j in range(PREDICT_LEN)]\n",
        "  current_list = []\n",
        "  'one sentence for one batch'\n",
        "  this_batch_prob = 0.\n",
        "  for index in range(len(p)):\n",
        "    'just get the character generated'\n",
        "    val = p[index]\n",
        "    cur_prob = np.log(p_prob[index])\n",
        "    if index == 0:\n",
        "      val = val[0]\n",
        "    current_char = n_to_char[val]\n",
        "    current_list.append(current_char)\n",
        "    this_batch_prob += cur_prob\n",
        "    if current_char == '\\n':\n",
        "      break\n",
        "  'we also wanna the average prob here'\n",
        "  this_batch_prob/=len(current_list)\n",
        "  current_list.remove('eos')\n",
        "  generated = ''.join(current_list)  # Convert back to text\n",
        "  if generated != '\\n':\n",
        "    generated_whole_list.append([this_batch_prob, generated])\n",
        "fin_res = sorted(generated_whole_list, key=lambda tup:tup[0], reverse=True)\n",
        "\n",
        "for this_batch_prob, generated in fin_res:\n",
        "  print(\"with prob: {}, generated: {}\".format(this_batch_prob, generated))\n",
        "# assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load in texts...\n",
            "VTB\n",
            "是的\n",
            "vtb是啥？\n",
            "emmm\n",
            "VTB：虚拟主播\n",
            "vtb是虚拟主播\n",
            "正解\n",
            "关注啦\n",
            "同样\n",
            "心疼\n",
            "貌似是神回\n",
            "只看到一些\n",
            "的吗？\n",
            "你猜\n",
            "不猜\n",
            "才能用\n",
            "不猜那就没有\n",
            "？？？？？？\n",
            "辣你还要怎么样嘛\n",
            "awsl\n",
            "awsl\n",
            "the previous text is from: 白上吹雪Official\n",
            "generating text...\n",
            "with prob: -0.5205316958191967, generated: 一个B坷垃 『奈喵』勋章带回家(〃'▽\n",
            "with prob: -0.5205317087335364, generated: 一个B坷垃 『奈喵』勋章带回家(〃'▽\n",
            "with prob: -1.0295778383811316, generated: 。。。。。。。。。。。。。。\n",
            "with prob: -1.1428002879133659, generated: 吧(=・ω・=)\n",
            "\n",
            "with prob: -1.1799932178575545, generated: ————————\n",
            "\n",
            "with prob: -1.2929939657449723, generated: 。。。\n",
            "\n",
            "with prob: -1.4150353372097015, generated: 有什么说法吗\n",
            "\n",
            "with prob: -1.4207580536603928, generated: 好难受啊\n",
            "\n",
            "with prob: -1.444367777556181, generated: 吗？\n",
            "\n",
            "with prob: -1.4443678110837936, generated: 吗？\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4626126885414124, generated: 吧\n",
            "\n",
            "with prob: -1.4971587434411049, generated: 挺好的\n",
            "\n",
            "with prob: -1.5364206979672115, generated: ！！！！\n",
            "\n",
            "with prob: -1.6172418229592342, generated: 挺简单的吧\n",
            "\n",
            "with prob: -1.6332188248634338, generated: ？\n",
            "\n",
            "with prob: -1.6332188248634338, generated: ？\n",
            "\n",
            "with prob: -1.6332188248634338, generated: ？\n",
            "\n",
            "with prob: -1.6332188248634338, generated: ？\n",
            "\n",
            "with prob: -1.6332188546657562, generated: ？\n",
            "\n",
            "with prob: -1.6332188546657562, generated: ？\n",
            "\n",
            "with prob: -1.6346709094941616, generated: 关了吧\n",
            "\n",
            "with prob: -1.6626860201358795, generated: ？？\n",
            "\n",
            "with prob: -1.6757602930068969, generated: 有什么区别\n",
            "\n",
            "with prob: -1.6758537404239178, generated: 吧？\n",
            "\n",
            "with prob: -1.681876282606806, generated: 是啥。。。\n",
            "\n",
            "with prob: -1.700668425609668, generated: 有录播吗？\n",
            "\n",
            "with prob: -1.709007942676544, generated: 是B限\n",
            "\n",
            "with prob: -1.7480293611685436, generated: 有什么意义吗\n",
            "\n",
            "with prob: -1.7601440727710724, generated: 很难受啊\n",
            "\n",
            "with prob: -1.7692815065383911, generated: 没啥用\n",
            "\n",
            "with prob: -1.7812618253131707, generated: 还是挺好的\n",
            "\n",
            "with prob: -1.7820369958877564, generated: 啥都没有\n",
            "\n",
            "with prob: -1.7822416685521603, generated: 啥？\n",
            "\n",
            "with prob: -1.7822416685521603, generated: 啥？\n",
            "\n",
            "with prob: -1.7867679099241893, generated: 什么时候都有\n",
            "\n",
            "with prob: -1.7887207865715027, generated: 不需要\n",
            "\n",
            "with prob: -1.7918918207287788, generated: 还行\n",
            "\n",
            "with prob: -1.8202848106622695, generated: 去404吧\n",
            "\n",
            "with prob: -1.8213453054428101, generated: 不需要的\n",
            "\n",
            "with prob: -1.8342776596546173, generated: 难受的\n",
            "\n",
            "with prob: -1.8417464246352513, generated: 什么时候高考啊\n",
            "\n",
            "with prob: -1.8502471049626668, generated: 吗\n",
            "\n",
            "with prob: -1.8502471049626668, generated: 吗\n",
            "\n",
            "with prob: -1.8502471049626668, generated: 吗\n",
            "\n",
            "with prob: -1.8523882418870925, generated: 比较好吧\n",
            "\n",
            "with prob: -1.879624257485072, generated: 啊\n",
            "\n",
            "with prob: -1.8796242872873943, generated: 啊\n",
            "\n",
            "with prob: -1.89723539352417, generated: 有啥说法\n",
            "\n",
            "with prob: -1.906225323677063, generated: 的\n",
            "\n",
            "with prob: -1.9087428092956542, generated: 有什么事\n",
            "\n",
            "with prob: -1.936538854241371, generated: 难受的啊\n",
            "\n",
            "with prob: -1.9429648108780384, generated: 全部木大\n",
            "\n",
            "with prob: -1.9587320923805236, generated: b限？\n",
            "\n",
            "with prob: -1.964343512058258, generated: 吗？？\n",
            "\n",
            "with prob: -1.9662608224898577, generated: 看的挺重要的吧\n",
            "\n",
            "with prob: -1.974848747253418, generated: 有的\n",
            "\n",
            "with prob: -1.979727270702521, generated: 比较好吗？\n",
            "\n",
            "with prob: -1.9863104701042176, generated: 有狐狸的\n",
            "\n",
            "with prob: -1.987627599388361, generated: 重开吧\n",
            "\n",
            "with prob: -1.9969298541545868, generated: 什么时候开始看\n",
            "\n",
            "with prob: -2.0051942405601344, generated: 不是日本人？\n",
            "\n",
            "with prob: -2.040031772106886, generated: b站高考的时候都是B限吧\n",
            "\n",
            "with prob: -2.0573124488194785, generated: 难受\n",
            "\n",
            "with prob: -2.0623194772217954, generated: 用的什么来着\n",
            "\n",
            "with prob: -2.062800705432892, generated: 6号\n",
            "\n",
            "with prob: -2.063783181210359, generated: 那是啥啊\n",
            "\n",
            "with prob: -2.064613321702135, generated: 晒熟肉吧(=・ω・=)\n",
            "\n",
            "with prob: -2.0715181955269406, generated: 有问题吗……\n",
            "\n",
            "with prob: -2.0866485039393106, generated: 都没看吧\n",
            "\n",
            "with prob: -2.0990926027297974, generated: 发的吧\n",
            "\n",
            "with prob: -2.10079163312912, generated: 还是有的\n",
            "\n",
            "with prob: -2.1009382754564285, generated: 难受吧\n",
            "\n",
            "with prob: -2.1044517923146486, generated: 。。\n",
            "\n",
            "with prob: -2.1100648045539856, generated: 说啥\n",
            "\n",
            "with prob: -2.111288830637932, generated: 都要去吧\n",
            "\n",
            "with prob: -2.133595734834671, generated: 没办法\n",
            "\n",
            "with prob: -2.1371845483779905, generated: 都没看到\n",
            "\n",
            "with prob: -2.137398878733317, generated: 不是这样的吧\n",
            "\n",
            "with prob: -2.1721576511859895, generated: 是啥呀\n",
            "\n",
            "with prob: -2.173102520406246, generated: 到底是谁？\n",
            "\n",
            "with prob: -2.174808330833912, generated: 咋回事\n",
            "\n",
            "with prob: -2.189097076654434, generated: 有啊\n",
            "\n",
            "with prob: -2.189097076654434, generated: 有啊\n",
            "\n",
            "with prob: -2.196674942970276, generated: 啥情况\n",
            "\n",
            "with prob: -2.1987075340002775, generated: 可以过。。。。\n",
            "\n",
            "with prob: -2.2079120099544527, generated: 没看过啊\n",
            "\n",
            "with prob: -2.2138117204109826, generated: 什么时候来辣\n",
            "\n",
            "with prob: -2.231232134951117, generated: 朋友(ಡωಡ)hiahiahia\n",
            "\n",
            "with prob: -2.2422636449337006, generated: 不是狐狸\n",
            "\n",
            "with prob: -2.248993766307831, generated: 就是很难受\n",
            "\n",
            "with prob: -2.277802363038063, generated: 不行么？。。。\n",
            "\n",
            "with prob: -2.2917399406433105, generated: 去哪里\n",
            "\n",
            "with prob: -2.297588622570038, generated: 有几率的\n",
            "\n",
            "with prob: -2.303851862748464, generated: 都要恰饭的\n",
            "\n",
            "with prob: -2.3273382981618247, generated: 没有\n",
            "\n",
            "with prob: -2.3273382981618247, generated: 没有\n",
            "\n",
            "with prob: -2.3404044147048677, generated: 也没听清吧\n",
            "\n",
            "with prob: -2.3761692494153976, generated: 在做什么呢\n",
            "\n",
            "with prob: -2.3764454821745553, generated: 都是好吗\n",
            "\n",
            "with prob: -2.377768483757973, generated: 看一下呗\n",
            "\n",
            "with prob: -2.3789872713387012, generated: 难啊\n",
            "\n",
            "with prob: -2.3953095078468323, generated: 有什么烤的啊\n",
            "\n",
            "with prob: -2.408832024782896, generated: 看狐狸吗？\n",
            "\n",
            "with prob: -2.416669115424156, generated: 还没直播？\n",
            "\n",
            "with prob: -2.41730335354805, generated: 没啥意思\n",
            "\n",
            "with prob: -2.420469726125399, generated: 不是韩国人吗\n",
            "\n",
            "with prob: -2.43239064514637, generated: 一样吧\n",
            "\n",
            "with prob: -2.4398999586701393, generated: 什么都没吧\n",
            "\n",
            "with prob: -2.4402074068784714, generated: 嘛，，。。。\n",
            "\n",
            "with prob: -2.4414047827847183, generated: ヽ(`Д´)ﾉ寻思\n",
            "\n",
            "with prob: -2.442219162529165, generated: 都不懂搞的啊。。。\n",
            "\n",
            "with prob: -2.452160432934761, generated: ...\n",
            "\n",
            "with prob: -2.452512353658676, generated: 是真的难\n",
            "\n",
            "with prob: -2.4526779651641846, generated: 很简单\n",
            "\n",
            "with prob: -2.4550175964832306, generated: 本来就是国际VTB嘛\n",
            "\n",
            "with prob: -2.462294268608093, generated: 没开的\n",
            "\n",
            "with prob: -2.4739037603139877, generated: 应该不需要设置吧\n",
            "\n",
            "with prob: -2.4767687797546385, generated: 没啥特殊\n",
            "\n",
            "with prob: -2.4843558371067047, generated: 吧（\n",
            "\n",
            "with prob: -2.502041310071945, generated: 应该没有\n",
            "\n",
            "with prob: -2.5085072927176952, generated: 正常？\n",
            "\n",
            "with prob: -2.5124565809965134, generated: 没啥歌的\n",
            "\n",
            "with prob: -2.517701178789139, generated: 可以去国内看。。。\n",
            "\n",
            "with prob: -2.533223509788513, generated: 没的\n",
            "\n",
            "with prob: -2.5332235991954803, generated: 没的\n",
            "\n",
            "with prob: -2.5404501110315323, generated: 不就行了\n",
            "\n",
            "with prob: -2.5440836027264595, generated: 不能放的吧\n",
            "\n",
            "with prob: -2.551800787448883, generated: 的歌\n",
            "\n",
            "with prob: -2.5522281229496, generated: B限吗（\n",
            "\n",
            "with prob: -2.572749824686484, generated: ，有啥服啊。。。。\n",
            "\n",
            "with prob: -2.582107365131378, generated: 怎么办啊\n",
            "\n",
            "with prob: -2.594984841346741, generated: 应该这样吧\n",
            "\n",
            "with prob: -2.5970134933789573, generated: 给我康康的\n",
            "\n",
            "with prob: -2.6036894534315382, generated: 家的时候难受啊\n",
            "\n",
            "with prob: -2.6101178765296935, generated: 大概不一样\n",
            "\n",
            "with prob: -2.6366502344608307, generated: 也有下的\n",
            "\n",
            "with prob: -2.652425453066826, generated: 吧，不会吧\n",
            "\n",
            "with prob: -2.6545305401086807, generated: 卡吧\n",
            "\n",
            "with prob: -2.655730565743787, generated: 没有平台的。。\n",
            "\n",
            "with prob: -2.6558192571004233, generated:  \n",
            "\n",
            "with prob: -2.6565486431121825, generated: 都是限定\n",
            "\n",
            "with prob: -2.6647029519081116, generated: 会死\n",
            "\n",
            "with prob: -2.6657161000702114, generated: 不知道有没有谁问的啊\n",
            "\n",
            "with prob: -2.6684350030762807, generated: 应该是国际游戏嘛\n",
            "\n",
            "with prob: -2.669203285127878, generated: B站的游戏真的难受\n",
            "\n",
            "with prob: -2.6776368183394275, generated: 需要转播man?\n",
            "\n",
            "with prob: -2.6791459739208223, generated: 问什么情况\n",
            "\n",
            "with prob: -2.7076939940452576, generated: 有版权的时间\n",
            "\n",
            "with prob: -2.7080286145210266, generated: 不会日语\n",
            "\n",
            "with prob: -2.708825225631396, generated: 不是21点半吗\n",
            "\n",
            "with prob: -2.7166285794228315, generated: 有什么平台播的事\n",
            "\n",
            "with prob: -2.7179030776023865, generated: 有么\n",
            "\n",
            "with prob: -2.721302105113864, generated: 为什么不长号啊？\n",
            "\n",
            "with prob: -2.722229427099228, generated: 直播是高考的时候。。。\n",
            "\n",
            "with prob: -2.7222408175468447, generated: 6号号\n",
            "\n",
            "with prob: -2.7249319702386856, generated: 难受?\n",
            "\n",
            "with prob: -2.73606476187706, generated: 不用来\n",
            "\n",
            "with prob: -2.7377246111631393, generated: 两个人啊\n",
            "\n",
            "with prob: -2.750859340031942, generated: 问一下一下的\n",
            "\n",
            "with prob: -2.753276512026787, generated: 够多\n",
            "\n",
            "with prob: -2.7579762935638428, generated: 不要停下来\n",
            "\n",
            "with prob: -2.758758459240198, generated: 更好吧\n",
            "\n",
            "with prob: -2.7764009051024914, generated: 干什么啊\n",
            "\n",
            "with prob: -2.7781229444912503, generated: 来的时候一定来\n",
            "\n",
            "with prob: -2.7875076234340668, generated: 这么长\n",
            "\n",
            "with prob: -2.8021349608898163, generated: 的阿\n",
            "\n",
            "with prob: -2.8045735699789867, generated: 哪里都没这样的\n",
            "\n",
            "with prob: -2.8078125218550363, generated: 1级都没用\n",
            "\n",
            "with prob: -2.8125040411949156, generated: 3点半啊\n",
            "\n",
            "with prob: -2.8178444632462094, generated: 高考的是如何吗\n",
            "\n",
            "with prob: -2.8205725799004235, generated: 不是B限，不然才八点\n",
            "\n",
            "with prob: -2.822124308347702, generated: 去去？\n",
            "\n",
            "with prob: -2.824468694627285, generated: 靠吧\n",
            "\n",
            "with prob: -2.8275863736867906, generated: 没过吧\n",
            "\n",
            "with prob: -2.8340410559127727, generated: 亏了一下……\n",
            "\n",
            "with prob: -2.836179440220197, generated: 到处都是啥\n",
            "\n",
            "with prob: -2.841311201453209, generated: 有人小心吗？\n",
            "\n",
            "with prob: -2.843203270435333, generated: 难。。\n",
            "\n",
            "with prob: -2.8474326547649174, generated: 现在也可以看录播。。\n",
            "\n",
            "with prob: -2.865515204767386, generated: 有多懂啊\n",
            "\n",
            "with prob: -2.871003565688928, generated: 有多棒啊\n",
            "\n",
            "with prob: -2.876541862885157, generated: 重开的不对\n",
            "\n",
            "with prob: -2.885193921625614, generated: 朋友吗\n",
            "\n",
            "with prob: -2.885298786063989, generated: 不是刷什么？\n",
            "\n",
            "with prob: -2.8879669904708862, generated: 心心\n",
            "\n",
            "with prob: -2.892483572165171, generated: 本来什么都听不懂\n",
            "\n",
            "with prob: -2.8987320264180503, generated: 够难受！！\n",
            "\n",
            "with prob: -2.900761842727661, generated: 没事么\n",
            "\n",
            "with prob: -2.9088034629821777, generated: 的，没找到\n",
            "\n",
            "with prob: -2.9220655858516693, generated: 就是正确吧\n",
            "\n",
            "with prob: -2.92825071811676, generated: 是国际小号\n",
            "\n",
            "with prob: -2.941733941435814, generated: 或者平台来的吧。\n",
            "\n",
            "with prob: -2.9425748189290366, generated: fbk\n",
            "\n",
            "with prob: -2.9448397494852543, generated: 怎么屏蔽阿？\n",
            "\n",
            "with prob: -2.9578713327646255, generated: 在高考\n",
            "\n",
            "with prob: -2.962536334991455, generated: 只有8号\n",
            "\n",
            "with prob: -2.9746702909469604, generated: 搞的事\n",
            "\n",
            "with prob: -2.9753835201263428, generated: 演的好吵\n",
            "\n",
            "with prob: -2.9890581145882607, generated: 烤？\n",
            "\n",
            "with prob: -2.9904395739237466, generated: 好\n",
            "\n",
            "with prob: -3.005704978747027, generated: 有录屏投稿吗？\n",
            "\n",
            "with prob: -3.0080381631851196, generated: 自己剪一下\n",
            "\n",
            "with prob: -3.0095819607377052, generated: 完美都是有东西的\n",
            "\n",
            "with prob: -3.021173079808553, generated: 多\n",
            "\n",
            "with prob: -3.0344734281301498, generated: 不是烤肉？\n",
            "\n",
            "with prob: -3.0508151700804857, generated: 代表的话题也很难受（悲）\n",
            "\n",
            "with prob: -3.0544936418533326, generated: 是 啊\n",
            "\n",
            "with prob: -3.056641936302185, generated: 怎么没有新衣服\n",
            "\n",
            "with prob: -3.070220583677292, generated: 录吧...\n",
            "\n",
            "with prob: -3.0702476104100547, generated: 都行\n",
            "\n",
            "with prob: -3.0743692006383623, generated: 的希望是小狐狸\n",
            "\n",
            "with prob: -3.0754925571382046, generated: 也能跟我有关吗\n",
            "\n",
            "with prob: -3.076023501157761, generated: 不好弄吧\n",
            "\n",
            "with prob: -3.0793882980942726, generated: 也不重要，不适应\n",
            "\n",
            "with prob: -3.0831585824489594, generated: 日语有什么难受\n",
            "\n",
            "with prob: -3.0884855100885034, generated: 不少的池子是什么？\n",
            "\n",
            "with prob: -3.096256709098816, generated: 有录播，应该在看烤肉吧\n",
            "\n",
            "with prob: -3.096574693918228, generated: poi\n",
            "\n",
            "with prob: -3.098127556698663, generated: 为什么会这么可爱吗\n",
            "\n",
            "with prob: -3.1011265886947514, generated: 不用设置比较难受的样子……\n",
            "\n",
            "with prob: -3.109918773174286, generated: 吵一下\n",
            "\n",
            "with prob: -3.11655609558026, generated: 聊一下的啊\n",
            "\n",
            "with prob: -3.120220348238945, generated: 演唱会啊\n",
            "\n",
            "with prob: -3.161182979742686, generated: 什么时候三个小时\n",
            "\n",
            "with prob: -3.1624859968821206, generated: 录\n",
            "\n",
            "with prob: -3.163349725306034, generated: 云玩家不需要正常操作\n",
            "\n",
            "with prob: -3.163617216878467, generated: 有国际VTB的也可以啊\n",
            "\n",
            "with prob: -3.1952436342835426, generated: 天啊\n",
            "\n",
            "with prob: -3.1971463646207536, generated: 看的什么会看\n",
            "\n",
            "with prob: -3.2111640237271786, generated: 看SC吗？\n",
            "\n",
            "with prob: -3.2300921231508255, generated: 都要犹豫的吧\n",
            "\n",
            "with prob: -3.2379850409924984, generated: fgo了解一下\n",
            "\n",
            "with prob: -3.2446808218955994, generated: 或者404\n",
            "\n",
            "with prob: -3.2500256299972534, generated: 不错\n",
            "\n",
            "with prob: -3.2520229518413544, generated: 或者重播\n",
            "\n",
            "with prob: -3.2574467957019806, generated: 去别人吧\n",
            "\n",
            "with prob: -3.2726942896842957, generated: 来玩\n",
            "\n",
            "with prob: -3.274788185954094, generated: 家么？\n",
            "\n",
            "with prob: -3.2798075497150423, generated: 不能进攻啊\n",
            "\n",
            "with prob: -3.2808482817241122, generated: 看个不这么多的\n",
            "\n",
            "with prob: -3.281489598751068, generated: 留言就可以\n",
            "\n",
            "with prob: -3.2821048498153687, generated: 都得烤\n",
            "\n",
            "with prob: -3.2839175264040628, generated: 有什么空嘛\n",
            "\n",
            "with prob: -3.287525348365307, generated: 都能让b站搜\n",
            "\n",
            "with prob: -3.2887377192576728, generated: 有啥剪辑呢\n",
            "\n",
            "with prob: -3.2905011102557182, generated: 什么时候来比较长（\n",
            "\n",
            "with prob: -3.2949972674250603, generated: 也可以愉快啊？\n",
            "\n",
            "with prob: -3.3041061729192736, generated: 醒醒啊？\n",
            "\n",
            "with prob: -3.3069042364756265, generated: 是什么那游戏\n",
            "\n",
            "with prob: -3.3089547157287598, generated: 关注不一定\n",
            "\n",
            "with prob: -3.3089871803919473, generated: 没版权都还没看嘛\n",
            "\n",
            "with prob: -3.3260652981698513, generated: 好菜吧\n",
            "\n",
            "with prob: -3.3289029465781317, generated: 狗狗不是人，应该是人\n",
            "\n",
            "with prob: -3.32940274477005, generated: 当初应该比较难受的\n",
            "\n",
            "with prob: -3.330800487101078, generated: 打不过233333\n",
            "\n",
            "with prob: -3.3412619531154633, generated: 也不没吗\n",
            "\n",
            "with prob: -3.3425054877996443, generated: 高考的时候更想搞很难受\n",
            "\n",
            "with prob: -3.348563492298126, generated: 第2的对吧\n",
            "\n",
            "with prob: -3.3618819415569305, generated: 什么时候没有听懂\n",
            "\n",
            "with prob: -3.362961769104004, generated: 睡觉\n",
            "\n",
            "with prob: -3.367153976112604, generated: ，可不是什么时候挑战的\n",
            "\n",
            "with prob: -3.3771996051073074, generated: 应该没什么回答的\n",
            "\n",
            "with prob: -3.3845467964808145, generated: 我好像看啊\n",
            "\n",
            "with prob: -3.3885002499446273, generated: 有的至少是14号……\n",
            "\n",
            "with prob: -3.3896402716636658, generated: 明天还有\n",
            "\n",
            "with prob: -3.3951372146606444, generated: 没必要爆炸\n",
            "\n",
            "with prob: -3.3984459042549133, generated: 另一个\n",
            "\n",
            "with prob: -3.4024446001276374, generated: 没有狐狸能过嘛？\n",
            "\n",
            "with prob: -3.4026084020733833, generated: 要吃到？\n",
            "\n",
            "with prob: -3.414774703979492, generated: 不是中文（\n",
            "\n",
            "with prob: -3.427701883018017, generated: 网易好一点啊\n",
            "\n",
            "with prob: -3.4279590845108032, generated: 去打游戏\n",
            "\n",
            "with prob: -3.431132560968399, generated: 加高级吗\n",
            "\n",
            "with prob: -3.449477434158325, generated: 不出去就行了\n",
            "\n",
            "with prob: -3.457015812397003, generated: 号没有\n",
            "\n",
            "with prob: -3.461661101396506, generated: 居然没有调香师\n",
            "\n",
            "with prob: -3.4700831969579062, generated: 可惜\n",
            "\n",
            "with prob: -3.470687544345856, generated: 就是30度\n",
            "\n",
            "with prob: -3.474442442258199, generated: 好点\n",
            "\n",
            "with prob: -3.4844301734119654, generated: 好孩子不一定会有的\n",
            "\n",
            "with prob: -3.490204925338427, generated: 要提前爆炸吗\n",
            "\n",
            "with prob: -3.491384188334147, generated: 长\n",
            "\n",
            "with prob: -3.4923266172409058, generated: 404的方法\n",
            "\n",
            "with prob: -3.4947865307331085, generated: 帮忙的朋友吧？\n",
            "\n",
            "with prob: -3.49703473970294, generated: 小可爱啊\n",
            "\n",
            "with prob: -3.4989977528651557, generated: 404是录像吗\n",
            "\n",
            "with prob: -3.502402186393738, generated: 有平台难受\n",
            "\n",
            "with prob: -3.506171335776647, generated: 就是这样的密码\n",
            "\n",
            "with prob: -3.5124366134405136, generated: 都行呀\n",
            "\n",
            "with prob: -3.5248423417409263, generated: 很快\n",
            "\n",
            "with prob: -3.52860364317894, generated: 怕是不是不一定看的事\n",
            "\n",
            "with prob: -3.5290822088718414, generated: 你要重开呢\n",
            "\n",
            "with prob: -3.541262497007847, generated: 挺受的\n",
            "\n",
            "with prob: -3.5415886975824833, generated: 没啥好聪明呀？\n",
            "\n",
            "with prob: -3.543973130484422, generated: 下午可以来啊\n",
            "\n",
            "with prob: -3.5512141784032187, generated: 日本都可以解决\n",
            "\n",
            "with prob: -3.558260332379076, generated: 最难的虎牙是什么操作？\n",
            "\n",
            "with prob: -3.5598837775843486, generated: 虚拟主播还需要游戏\n",
            "\n",
            "with prob: -3.5644840955734254, generated: 那里找什么\n",
            "\n",
            "with prob: -3.565041720867157, generated: 过不去的\n",
            "\n",
            "with prob: -3.5653699661294618, generated: 让她转圈吧，我听的好快\n",
            "\n",
            "with prob: -3.57649294535319, generated: 不是主要或者的\n",
            "\n",
            "with prob: -3.5832231521606444, generated: 现实好难受\n",
            "\n",
            "with prob: -3.5845031797885896, generated: 赚没用的\n",
            "\n",
            "with prob: -3.5879877976008823, generated: 支持他们看的吗\n",
            "\n",
            "with prob: -3.5910660127798715, generated: 也没事完美的\n",
            "\n",
            "with prob: -3.5913268451889357, generated: 不是人气要啊\n",
            "\n",
            "with prob: -3.5919932467596873, generated: 是200以下的VTB\n",
            "\n",
            "with prob: -3.599307805299759, generated: 还有一层\n",
            "\n",
            "with prob: -3.6111203928788504, generated: 国际vtb属于看\n",
            "\n",
            "with prob: -3.6155044436454773, generated: 砍不到\n",
            "\n",
            "with prob: -3.6292834877967834, generated: 就是看不了啦\n",
            "\n",
            "with prob: -3.634521742661794, generated: 多也要留下\n",
            "\n",
            "with prob: -3.6405228711664677, generated: 看不到的还不解决呢\n",
            "\n",
            "with prob: -3.64126283592648, generated: 难的时候都加不到200\n",
            "\n",
            "with prob: -3.6422974944114683, generated: 一下的怎么样\n",
            "\n",
            "with prob: -3.644974758848548, generated: 该捕捉的（）\n",
            "\n",
            "with prob: -3.6459549833089113, generated: 一个亿一下留言！！！\n",
            "\n",
            "with prob: -3.6480008222990565, generated: 不不能被榨干吗。。\n",
            "\n",
            "with prob: -3.6510578269759812, generated: 都能大丈夫？\n",
            "\n",
            "with prob: -3.654549633463224, generated: 国内不是小学啊\n",
            "\n",
            "with prob: -3.6562844150596194, generated: 是自己的可以留下的地方\n",
            "\n",
            "with prob: -3.661046952009201, generated: 早点复习\n",
            "\n",
            "with prob: -3.6610982567071915, generated: 需要mio的游戏吧\n",
            "\n",
            "with prob: -3.66750921521868, generated: 比的好看太多\n",
            "\n",
            "with prob: -3.6797920055687428, generated: 刺身?\n",
            "\n",
            "with prob: -3.691877620560782, generated: 3d没版权吧（\n",
            "\n",
            "with prob: -3.698197662830353, generated: 必须去搞啥\n",
            "\n",
            "with prob: -3.7018389105796814, generated: 能等\n",
            "\n",
            "with prob: -3.7160247445106505, generated: 不加而已\n",
            "\n",
            "with prob: -3.726572275161743, generated: 没法重复\n",
            "\n",
            "with prob: -3.7298542422552905, generated: 还是无能号？\n",
            "\n",
            "with prob: -3.7341330614354877, generated: 好气的吖！啊啊啊啊啊啊啊\n",
            "\n",
            "with prob: -3.7385344675609042, generated: 不是B站,命运\n",
            "\n",
            "with prob: -3.7398241087794304, generated: 压力一点就是爆炸。。\n",
            "\n",
            "with prob: -3.7457255363464355, generated: 本来在解释\n",
            "\n",
            "with prob: -3.7501373561945828, generated: 啊，有狐狸是这么晚的吧\n",
            "\n",
            "with prob: -3.7516565195151736, generated: 肯定为什么没有这个号\n",
            "\n",
            "with prob: -3.75203142563502, generated: 打工的时候见\n",
            "\n",
            "with prob: -3.762134701013565, generated: 狐狸的雪山去播啊\n",
            "\n",
            "with prob: -3.763399749994278, generated: 只是没开\n",
            "\n",
            "with prob: -3.7804768085479736, generated: 的时候概率\n",
            "\n",
            "with prob: -3.7822000980377197, generated: 才应该是属于\n",
            "\n",
            "with prob: -3.7835244953632357, generated: 比较属于难受\n",
            "\n",
            "with prob: -3.7841427760819593, generated: 有的东西度应该是来着一会儿吧\n",
            "\n",
            "with prob: -3.788724252155849, generated: 对的，还有敏感\n",
            "\n",
            "with prob: -3.8010776386811185, generated: 高考的时候能就是想着去睡觉的鉴啊\n",
            "\n",
            "with prob: -3.806017827987671, generated: 一定不会升级\n",
            "\n",
            "with prob: -3.815187838114798, generated: 才是为了放松直播（高考）\n",
            "\n",
            "with prob: -3.8396284878253937, generated: 无脑有多高\n",
            "\n",
            "with prob: -3.843244045972824, generated: 问一下战队 啊\n",
            "\n",
            "with prob: -3.850812340620905, generated: 重复不同不好吗……\n",
            "\n",
            "with prob: -3.85708572268486, generated: 和国际主播联动都就不会动\n",
            "\n",
            "with prob: -3.8646421432495117, generated: 少主的不少\n",
            "\n",
            "with prob: -3.8672558665275574, generated:  可以刷熟肉\n",
            "\n",
            "with prob: -3.8721136331558226, generated: 20度不高\n",
            "\n",
            "with prob: -3.8767999559640884, generated: 不能看bug的类型的\n",
            "\n",
            "with prob: -3.877774015069008, generated: 可以刚好\n",
            "\n",
            "with prob: -3.884633575166975, generated: 都有烤肉片 \n",
            "\n",
            "with prob: -3.893817428838123, generated: 今年的狐狸还是为什么不晒的？\n",
            "\n",
            "with prob: -3.8949631690979003, generated: 我是不用\n",
            "\n",
            "with prob: -3.901448213391834, generated: ，咦今天什么时候准时啊\n",
            "\n",
            "with prob: -3.9067243138949075, generated: 倒是做好啊 有希望\n",
            "\n",
            "with prob: -3.9179535678454807, generated: 你自己会闲吗\n",
            "\n",
            "with prob: -3.9214661687612535, generated: 第一天干什么啊\n",
            "\n",
            "with prob: -3.9312098224957785, generated: ku\n",
            "\n",
            "with prob: -3.9349865913391113, generated: 写全部不难吗？\n",
            "\n",
            "with prob: -3.951937961578369, generated: 比较容易老实\n",
            "\n",
            "with prob: -3.956506941999708, generated: 不放假，骑士多\n",
            "\n",
            "with prob: -3.970546007156372, generated: 姆啊\n",
            "\n",
            "with prob: -3.9750672578811646, generated: 自动会员吗？(\n",
            "\n",
            "with prob: -3.983767577580043, generated: 日本那边都有些生活\n",
            "\n",
            "with prob: -3.98697030544281, generated: 小心考啥\n",
            "\n",
            "with prob: -3.9947005212306976, generated: 不剪不用出吗\n",
            "\n",
            "with prob: -4.003948211669922, generated: 的游戏那边好多 \n",
            "\n",
            "with prob: -4.023307204246521, generated: 满分\n",
            "\n",
            "with prob: -4.026817036526544, generated: 5对8就是算了\n",
            "\n",
            "with prob: -4.035299273473876, generated: 到时候也去动啊\n",
            "\n",
            "with prob: -4.053706211703164, generated: 表示不对听力吗\n",
            "\n",
            "with prob: -4.064769816895326, generated: 重开号确实难受\n",
            "\n",
            "with prob: -4.088425286114216, generated: fbk不拜拜啊\n",
            "\n",
            "with prob: -4.09222625060515, generated: 打飞机肯定至少挺好打的。\n",
            "\n",
            "with prob: -4.09958016872406, generated: 不是下午方便\n",
            "\n",
            "with prob: -4.105528333623495, generated: 嘤嘤嘤有脑子\n",
            "\n",
            "with prob: -4.118182004895061, generated: 上号堆分+1\n",
            "\n",
            "with prob: -4.1402872279286385, generated: 无所谓...\n",
            "\n",
            "with prob: -4.141053756078084, generated: 卡住\n",
            "\n",
            "with prob: -4.16054527759552, generated: 肯定有2w\n",
            "\n",
            "with prob: -4.177729328473409, generated: 让aki放什么\n",
            "\n",
            "with prob: -4.182168559895621, generated: 有意见那边有些好像没看\n",
            "\n",
            "with prob: -4.189858755717675, generated: 要不然不会游泳？\n",
            "\n",
            "with prob: -4.19082373380661, generated: 找不起\n",
            "\n",
            "with prob: -4.1911888182163235, generated: 趴的歌\n",
            "\n",
            "with prob: -4.206952720880508, generated: 30号几点玩\n",
            "\n",
            "with prob: -4.216160211298201, generated: 不把话题用啥说啥\n",
            "\n",
            "with prob: -4.223382765427232, generated: 不进来的话你可以吧\n",
            "\n",
            "with prob: -4.230295434594154, generated: 啥生物\n",
            "\n",
            "with prob: -4.2311589360237125, generated: girl,吧\n",
            "\n",
            "with prob: -4.242067456245422, generated: 最后是\n",
            "\n",
            "with prob: -4.2422661781311035, generated: 就比较复杂加的问题\n",
            "\n",
            "with prob: -4.251162074506283, generated: 应该要到其他地方说节目效果会怎样\n",
            "\n",
            "with prob: -4.268973760306835, generated: 需要锰矿\n",
            "\n",
            "with prob: -4.269635200500488, generated: 只是星期的坑都是本人\n",
            "\n",
            "with prob: -4.2749876864254475, generated: hh呗\n",
            "\n",
            "with prob: -4.280646828087893, generated: 不是在玩啥？是不是不是酋长。\n",
            "\n",
            "with prob: -4.2968566332544595, generated: 时间时不睡，B限无论的都都说\n",
            "\n",
            "with prob: -4.319325387477875, generated: 人气难受啥\n",
            "\n",
            "with prob: -4.323405528068543, generated: 都二日语的UP还是有点难受\n",
            "\n",
            "with prob: -4.330196609720588, generated: 我的号上课真正常\n",
            "\n",
            "with prob: -4.3388263781865435, generated: 2号不到她们\n",
            "\n",
            "with prob: -4.350207066535949, generated: 不想啊看\n",
            "\n",
            "with prob: -4.358687897523244, generated: 大脑大概不用做\n",
            "\n",
            "with prob: -4.377259349822998, generated: 去号好气\n",
            "\n",
            "with prob: -4.390095941722393, generated: 还是2233好看？\n",
            "\n",
            "with prob: -4.398458898067474, generated: 加上狐狸\n",
            "\n",
            "with prob: -4.39938143491745, generated: 凌晨高达吧\n",
            "\n",
            "with prob: -4.399582237005234, generated: C管吧\n",
            "\n",
            "with prob: -4.402910888195038, generated: 旁边有国内账号日本人\n",
            "\n",
            "with prob: -4.41432363986969, generated: 总是这么闲\n",
            "\n",
            "with prob: -4.423023283481598, generated: 狐一个\n",
            "\n",
            "with prob: -4.43767789999644, generated: 睡的久，上面八点呢\n",
            "\n",
            "with prob: -4.458434164524078, generated: 说的比较天啊难受\n",
            "\n",
            "with prob: -4.466668484302668, generated: 上。。。mea不懂 定都哭\n",
            "\n",
            "with prob: -4.47009539604187, generated: 不高就是余吧\n",
            "\n",
            "with prob: -4.497914465268453, generated: 应该是怪物 7号 不用复习 不然什么事下去\n",
            "with prob: -4.508409712049696, generated: 完全不会问再说，这边的\n",
            "\n",
            "with prob: -4.552549383857033, generated: 不知道国内的游戏感觉挺都精神\n",
            "\n",
            "with prob: -4.553435606615884, generated: 你以为是语号\n",
            "\n",
            "with prob: -4.564008183777332, generated: t台有水平T吗\n",
            "\n",
            "with prob: -4.581082534044981, generated: 上中国几率?\n",
            "\n",
            "with prob: -4.586007803678513, generated: 等等996？\n",
            "\n",
            "with prob: -4.599530903001626, generated: 有往下面？\n",
            "\n",
            "with prob: -4.639683488756418, generated: 你还不想睡着屠夫？\n",
            "\n",
            "with prob: -4.659557420015335, generated: 顺序尊重吧\n",
            "\n",
            "with prob: -4.6747095957398415, generated: 放个黑之类啊\n",
            "\n",
            "with prob: -4.677251166767544, generated: 寻思的，得弄什么唱过\n",
            "\n",
            "with prob: -4.683025141557057, generated: 都有打问网易音乐的\n",
            "\n",
            "with prob: -4.6848567468779425, generated: 看不见，only的歌\n",
            "\n",
            "with prob: -4.6903238743543625, generated: 觉醒都行\n",
            "\n",
            "with prob: -4.696909806945107, generated: 设置关了还是今年各种的不可能...\n",
            "\n",
            "with prob: -4.701299389203389, generated: 技能本来还来\n",
            "\n",
            "with prob: -4.74580080807209, generated: 一周属实，都不会打折\n",
            "\n",
            "with prob: -4.751208221912384, generated: 以下礼物呗\n",
            "\n",
            "with prob: -4.771996155381203, generated: 优化辣\n",
            "\n",
            "with prob: -4.798918199539185, generated: 也成功一个月\n",
            "\n",
            "with prob: -4.803172452109201, generated: 明年就专门卡看\n",
            "\n",
            "with prob: -4.819059097766877, generated: 大佬吃点吗\n",
            "\n",
            "with prob: -4.830444812774658, generated: 坐生肉不用\n",
            "\n",
            "with prob: -4.858009656270345, generated: 转播不是少主而已\n",
            "\n",
            "with prob: -4.8813483119010925, generated: 调插件\n",
            "\n",
            "with prob: -4.88274073600769, generated: nns的转播\n",
            "\n",
            "with prob: -4.888262828191121, generated: 不！那里哪里有朋友是\n",
            "\n",
            "with prob: -4.894220892339945, generated: 我们以后都可以man录播\n",
            "\n",
            "with prob: -4.906066169341405, generated: 可不还想到么\n",
            "\n",
            "with prob: -4.915057642592324, generated: 还笑谢拉的作战怎么看\n",
            "\n",
            "with prob: -4.9195102378726006, generated: n然后不会删弹幕吧\n",
            "\n",
            "with prob: -4.944417414388487, generated: 不再上场还不错...\n",
            "\n",
            "with prob: -4.946097373962402, generated: 为了无能poi\n",
            "\n",
            "with prob: -4.959775149822235, generated: 留下OKOK\n",
            "\n",
            "with prob: -4.98117995262146, generated: 高级分，问题\n",
            "\n",
            "with prob: -4.995608290036519, generated: 待明年，应该没用系系\n",
            "\n",
            "with prob: -5.001651815392754, generated: 合作甚至转播或者在国内平台都慌\n",
            "\n",
            "with prob: -5.003922899067402, generated: 对不到是不是操作混沌的世界啊\n",
            "\n",
            "with prob: -5.029115597407023, generated: 不对，，就是没什么问题，那fbk只是fbk急装\n",
            "with prob: -5.038323705846613, generated: DD不一样是10群，就是就是\n",
            "\n",
            "with prob: -5.038917944981502, generated: 妈我在偷偷看下视频，再说每天下班\n",
            "\n",
            "with prob: -5.059432208538055, generated: 夏活1日语没有打不过\n",
            "\n",
            "with prob: -5.071129132062197, generated: 这吊人自己说现充的吧\n",
            "\n",
            "with prob: -5.105339785416921, generated: 是女向那样\n",
            "\n",
            "with prob: -5.119299530982971, generated: 吃饭维护up？！\n",
            "\n",
            "with prob: -5.203227750957012, generated: 需要谁追 是排位\n",
            "\n",
            "with prob: -5.347774267196655, generated:  那个定反应\n",
            "\n",
            "with prob: -5.457780735833304, generated: 建议200人前，不敢看咱在说下午真\n",
            "\n",
            "with prob: -5.475813488165538, generated: 可以副V 不再来看，我也爱\n",
            "\n",
            "with prob: -5.694573336413929, generated: 下载会儿模拟器种子吧\n",
            "\n",
            "with prob: -5.717464736529759, generated: 发脏话平安也没用\n",
            "\n",
            "with prob: -5.99477031826973, generated: 举报增加高断气\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWsSJRL4-4Eb",
        "colab_type": "text"
      },
      "source": [
        "####Get the generated data & origin data for comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaWYpM7u-2W9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "52ee68db-6346-4c90-8893-ae72f0159676"
      },
      "source": [
        "origin_whole_list = []\n",
        "\n",
        "'get some origin data in a list'\n",
        "with open(preprocessed_TXT, encoding='UTF-8') as json_file:\n",
        "    data = json.load(json_file, encoding='UTF-8')\n",
        "\n",
        "'randomly sample some data from the list'\n",
        "index_range = np.arange(len(data))\n",
        "np.random.shuffle(index_range)\n",
        "\n",
        "'obtain the data'\n",
        "for index in range(PREDICT_LEN):\n",
        "  origin_data = ''.join(data[index_range[index]])\n",
        "  origin_whole_list.append(origin_data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7fa0b340f5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m'obtain the data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREDICT_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0morigin_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0morigin_whole_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBZ5easAlr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"real_message samples: {}\".format(origin_whole_list[:5]))\n",
        "print(\"fake_messages samples: {}\".format(generated_whole_list[:5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKzuCc9LA6vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'with pandas data frame'\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "df = pd.DataFrame(columns=['message', 'label'])\n",
        "for origin in origin_whole_list:\n",
        "  df = df.append({'message': origin, 'label': 1}, ignore_index=True)\n",
        "for fake in generated_whole_list:\n",
        "  df = df.append({'message': fake, 'label': 0}, ignore_index=True)\n",
        "df = shuffle(df)\n",
        "df.to_csv(\"/content/fake_danmaku_evaluation.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2a5cGsSTEBQD"
      },
      "source": [
        "## What's next\n",
        "\n",
        "* Danmaku-caption: generate danmaku based on context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL73nQwIO5yO",
        "colab_type": "text"
      },
      "source": [
        "### For TS-Javascript\n",
        "Load and save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPIYk5gTO3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction_model = lstm_model(seq_len=1,batch_size=8, stateful=True)\n",
        "# prediction_model.load_weights('/tmp/bard_{}.h5'.format(0))\n",
        "# prediction_model.save('/tmp/bard.js.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}