# -*- coding: utf-8 -*-
"""“Predict Danmaku with Cloud TPUs and Keras”

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSXNQ3IXjvLE2daqFVmdZvHDYn7Xzgkd

##### Copyright 2018 The TensorFlow Hub Authors.

Licensed under the Apache License, Version 2.0 (the "License");
"""

# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

## Predict vtuber danmaku with Cloud TPUs and Keras
#### Modified from "Predict Shakespeare with Cloud TPUs and Keras"
'Author github ID: pren1, coco401, simon3000, Afanyiyu'

from process_prepare import process_prepare
from model_builder import model_structure_loader
from generating_text import text_generator

if __name__ == '__main__':
	'the character should occur this much time if they wanna to be taken into account'
	context_vector_length = 100
	context_seq_length = 130
	BATCH_SIZE = 500
	PREDICT_LEN = 50
	model_folder = 'tmp'

	'prepare for the whole process'
	preparer = process_prepare(target_path_folder = 'content')
	model_builder = model_structure_loader(characters = preparer.characters, embedding_matrix = preparer.embedding_matrix, context_vector_length = context_vector_length)
	_, encoder_model, _ = model_builder.lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)
	encoder_model.load_weights('./{}/encoder.h5'.format(model_folder))
	decoder_model = model_builder.get_stand_alone_decoder(seq_len=1, batch_size=BATCH_SIZE, stateful=True)
	decoder_model.load_weights('./{}/decoder.h5'.format(model_folder))
	generator = text_generator(encoder_model, decoder_model, BATCH_SIZE, PREDICT_LEN, preparer)
	# We seed the model with our initial string, copied BATCH_SIZE times
	new_txt = preparer.load_in_texts()
	print("Load in texts...")
	seed = preparer.transform(preparer.clip_text(100, new_txt))
	print("generating text...")
	generator.predict_interface(seed)